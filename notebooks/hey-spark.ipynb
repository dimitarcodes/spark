{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting started with PySpark\n",
    "\n",
    "## Apache Spark\n",
    "\n",
    "Spark is software that is used to process large datasets. Each process is called a task and tasks can be distributed across multiple machines for parallel processing. Spark allows the usage of a bunch of languages to define and issue tasks, including Python, R, SQL and Java but we will be using Scala in this course.\n",
    "\n",
    "## RDD\n",
    "\n",
    "Spark works with **Resilient Distributed Datasets** or **RDD** for short. \n",
    "* Resilient because once an RDD has been created it cannot be changed, allowing for its quick reuse directly from memory instead of mass storage  (which makes them faster than map-reduce)\n",
    "* Distributed because they can be  distributed over multiple computers\n",
    "* Datasets because they hold data\n",
    "\n",
    "RDDs can be created using the **Spark Context** object `sc`.\n",
    "* using an already defined object, such as an array, with the function `parallelize`, 2nd argument is the number of workers\n",
    "    * `rdd = sc.parallelize(0 to 999, 8)`\n",
    "* using a file, with the function `textFile`, 2nd argument is again the number of workers\n",
    "    * `sc.textFile(\"file\",3)`\n",
    "\n",
    "It is important to note most of the time you might not need to actually use the SparkContext but the SparkSession instead. The difference between the two in short:\n",
    "\n",
    "* SparkSession:\n",
    "    * for working with high-level APIs such as DataFrame and SQL.\n",
    "    * Ideal for most data processing tasks because of its simplicity and unified interface.\n",
    "\n",
    "* SparkContext:\n",
    "    * low-level RDD operations.\n",
    "    * Necessary when you need more control over the underlying Spark execution or need to manipulate RDDs directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://172.17.0.1:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>heySpark</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7c1788fe05f0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "\n",
    "spark = SparkSession.builder.appName(\"heySpark\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version: 3.5.5\n"
     ]
    }
   ],
   "source": [
    "print(\"Spark version: %s\" % spark.version)\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lazy Evaluation\n",
    "Spark uses lazy evaluation, meaning that execution of functions takes place only when other functions need it.\n",
    "\n",
    "* for example ```example = sc.parallelize(range(999),8)``` defines how the variable `example` will be created, but it isn't actually evaluated and stored in memory until it is needed somewhere else, for example by the following line ```sample = example.takeSample(false, 4)``` which requires to evaluation of the variable `example` in orderto return samples from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize(range(999), 8) # input array + number of partitions of the new RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[227, 549, 267, 710]\n"
     ]
    }
   ],
   "source": [
    "sample = rdd.takeSample(0, 4)\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jobs and stages\n",
    "\n",
    "Jobs are essentially the queries that users submit when they run a command involving an RDD. Each job is divided into stages, with stages being stored in memory so that if another job requires the same stage it can be swiftly reused without extra computational cost. From the jobs tab you can see stats for each job, such as when it was submitted and how long it took to finish.\n",
    "\n",
    "Clicking on the job in the Spark UI provides us with a more detailed overview, including a directed acyclic graph showing the functions that comprise the job.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading text data\n",
    "Romeo & Juliet provided by Project Guthenberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget --quiet https://raw.githubusercontent.com/rubigdata-dockerhub/hadoop-dockerfile/master/100.txt # Guthenberg data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guthenberg text file contains:\n",
      "147838 lines\n",
      "5545144 chars\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lines = sc.textFile(\"data/guthenberg.txt\")\n",
    "print(\"Guthenberg text file contains:\\n\" + \n",
    "        \"%d lines\\n\" % lines.count() + \n",
    "        \"%d chars\\n\" % lines.map( lambda s: len(s) ).reduce(lambda a, b: a + b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longest line: 78 char\n"
     ]
    }
   ],
   "source": [
    "print(\"Longest line: %s char\" % lines.map( lambda s: len(s) ).reduce(lambda a, b: max(a,b)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the lines into words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect words\n",
    "words = lines.flatMap(lambda s: s.split()).filter(lambda w: len(w) > 0).map(lambda w: (w, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 25:>                                                         (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Complete', 4), ('Works', 5), ('of', 16718), ('by', 3064), ('Shakespeare', 18), ('eBook', 8), ('for', 6014), ('use', 286), ('anyone', 6), ('United', 15)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# reduce words collection to word count\n",
    "wc = words.reduceByKey(lambda a, b: a + b)\n",
    "print(wc.take(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2) PythonRDD[53] at RDD at PythonRDD.scala:53 []\n",
      " |  MapPartitionsRDD[51] at mapPartitions at PythonRDD.scala:160 []\n",
      " |  ShuffledRDD[50] at partitionBy at NativeMethodAccessorImpl.java:0 []\n",
      " +-(2) PairwiseRDD[49] at reduceByKey at /tmp/ipykernel_36535/1104536454.py:1 []\n",
      "    |  PythonRDD[48] at reduceByKey at /tmp/ipykernel_36535/1104536454.py:1 []\n",
      "    |  data/guthenberg.txt MapPartitionsRDD[36] at textFile at NativeMethodAccessorImpl.java:0 []\n",
      "    |  data/guthenberg.txt HadoopRDD[35] at textFile at NativeMethodAccessorImpl.java:0 []\n"
     ]
    }
   ],
   "source": [
    "print(wc.toDebugString().decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count words\n",
    "which words Shakespeare used most often?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('\"', 241), ('\"\\'Tis', 1), ('\"A', 3), ('\"Air,\"', 1), ('\"Alas,', 1), ('\"Amen\"', 2), ('\"Amen\"?', 1), ('\"Amen,\"', 1), ('\"And', 1), ('\"Aroint', 1)]\n",
      "[('Author:', 1), ('[EBook', 1), ('1,', 1), ('2018', 1), ('START', 1), ('KINSMEN', 1), ('Feedâ€™st', 1), ('fuel,', 1), ('cruel:', 1), ('buriest', 1)]\n",
      "[('the', 25378), ('I', 20629), ('and', 19806), ('to', 16966), ('of', 16718), ('a', 13657), ('my', 11443), ('in', 10519), ('you', 9591), ('is', 8335)]\n"
     ]
    }
   ],
   "source": [
    "top10word = wc.takeOrdered(10, key = lambda x: x[0])\n",
    "top10count = wc.takeOrdered(10, key = lambda x: x[1])\n",
    "top10countrev = wc.takeOrdered(10, key = lambda x: -x[1])\n",
    "print(top10word)\n",
    "print(top10count)\n",
    "print(top10countrev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Romeo', 45)]\n",
      "[('Juliet', 17)]\n"
     ]
    }
   ],
   "source": [
    "print(wc.filter(lambda x: x[0] == \"Romeo\").collect())\n",
    "print(wc.filter(lambda x: x[0] == \"Juliet\").collect())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[53] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wc.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Macbeth', 30)]\n",
      "[('Capulet', 9)]\n"
     ]
    }
   ],
   "source": [
    "print(wc.filter(lambda x: x[0] == \"Macbeth\").collect())\n",
    "print(wc.filter(lambda x: x[0] == \"Capulet\").collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alternative ways to achieve the same result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 25378), ('I', 20629), ('and', 19806), ('to', 16966), ('of', 16718), ('a', 13657), ('my', 11443), ('in', 10519), ('you', 9591), ('is', 8335)]\n"
     ]
    }
   ],
   "source": [
    "# method 1 of getting top 10 words\n",
    "print(wc.map(lambda x: (x[1], x[0])).sortByKey(ascending=False).map(lambda x: (x[1], x[0])).take(10)) # 0.3s according to the Spark UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 25378), ('I', 20629), ('and', 19806), ('to', 16966), ('of', 16718), ('a', 13657), ('my', 11443), ('in', 10519), ('you', 9591), ('is', 8335)]\n"
     ]
    }
   ],
   "source": [
    "# method 2 of getting top 10 words\n",
    "print(wc.takeOrdered(10, key = lambda x: -x[1])) # 99ms according to the Spark UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 25378), ('I', 20629), ('and', 19806), ('to', 16966), ('of', 16718), ('a', 13657), ('my', 11443), ('in', 10519), ('you', 9591), ('is', 8335)]\n"
     ]
    }
   ],
   "source": [
    "# method 3 of getting top 10 words\n",
    "print(wc.top(10, key = lambda x: x[1])) # 95ms according to the Spark UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 25378), ('I', 20629), ('and', 19806), ('to', 16966), ('of', 16718), ('a', 13657), ('my', 11443), ('in', 10519), ('you', 9591), ('is', 8335)]\n"
     ]
    }
   ],
   "source": [
    "# method 4 of getting top 10 words\n",
    "print(wc.sortBy(lambda x: -x[1]).take(10)) # 0.3s according to the Spark UI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the counted words\n",
    "\n",
    "The result is multiple files, a result of having multiple workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "words.saveAsTextFile(\"data/guthenberg-words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning up text data with regex\n",
    "\n",
    "MacBeth is actually mentioned many more times than we previously found, however our original way of splitting lines into words did not account for signs such as a fullstop and an exclamation mark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Macbeth', 30),\n",
       " ('Macbeth.', 20),\n",
       " ('MACBETH.', 205),\n",
       " ('Macbeth,', 16),\n",
       " ('Macbeth.]', 1),\n",
       " ('Macbeth!', 7),\n",
       " ('MACBETH', 2),\n",
       " ('MACBETH,', 2),\n",
       " (\"Macbeth's\", 7),\n",
       " ('Macbeth;', 2)]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wc = lines.flatMap(lambda s: s.split()).filter(lambda w: len(w) > 0).map(lambda w: (w, 1)).reduceByKey(lambda a, b: a + b)\n",
    "wc.filter(lambda x: \"macbeth\" in x[0].lower()).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To fix this we can use a regex to filter out any signs outside of the english alphabet (a-z)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "words = lines.flatMap(lambda l: l.split(\" \")) \\\n",
    "                .map(lambda w: re.sub( r\"(^[^a-z]+|[^a-z]+$)\", \"\", w.lower())) \\\n",
    "                .filter(lambda w: len(w) > 0) \\\n",
    "                .map(lambda w: (w, 1)) \\\n",
    "                .reduceByKey(lambda a, b: a + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 178:>                                                        (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macbeth count: 285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "mbcount = words.filter(lambda x: x[0] == \"macbeth\").collect()\n",
    "print(\"Macbeth count: %d\" % mbcount[0][1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dataeng",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
