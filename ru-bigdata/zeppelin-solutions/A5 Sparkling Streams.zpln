{
  "paragraphs": [
    {
      "text": "%md\n# Spark Structured Streaming\n\nSpark Structured Streaming offers query processing over dataframes in a streaming fashion. Before you start, read the excellent [introduction to Spark Structured Streaming](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html). The lab session further assumes that you have attended the Structured Streaming lecture (#8 part B) and have read the background paper (posted on Brightspace).\n\nThe exercise uses the abstractions offered by Spark Structured Streaming to analyze the data from an online marketplace - inspired by a popular online gam[e](https://www.youtube.com/watch?v=BJhF0L7pfo8).\n\nIn that game:\n\n+ players sell various items; whenever an item is sold the transaction is reported;\n+ every item has a material (e.g., Iron, Steel), a type (Sword, Shield), and a price;\n+ in order to get live updates on the economy, a Spark Streaming application would be perfect.\n\nJust like the previous lab sessions, work through the notebook by answering the questions posed, and implementing various features for your ideal game dashboard. \nUse the questions and the code you wrote as the basis for your blog post. Oh, and if you did not spot the easter-egg in this cell, you may want to look at the markdown source (^-alt-E).",
      "user": "anonymous",
      "dateUpdated": "2021-04-26T19:39:24+0000",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9,
        "editorHide": true,
        "results": {},
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h1>Spark Structured Streaming</h1>\n<p>Spark Structured Streaming offers query processing over dataframes in a streaming fashion. Before you start, read the excellent <a href=\"https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html\">introduction to Spark Structured Streaming</a>. The lab session further assumes that you have attended the Structured Streaming lecture (#8 part B) and have read the background paper (posted on Brightspace).</p>\n<p>The exercise uses the abstractions offered by Spark Structured Streaming to analyze the data from an online marketplace - inspired by a popular online gam<a href=\"https://www.youtube.com/watch?v=BJhF0L7pfo8\">e</a>.</p>\n<p>In that game:</p>\n<ul>\n<li>players sell various items; whenever an item is sold the transaction is reported;</li>\n<li>every item has a material (e.g., Iron, Steel), a type (Sword, Shield), and a price;</li>\n<li>in order to get live updates on the economy, a Spark Streaming application would be perfect.</li>\n</ul>\n<p>Just like the previous lab sessions, work through the notebook by answering the questions posed, and implementing various features for your ideal game dashboard.<br />\nUse the questions and the code you wrote as the basis for your blog post. Oh, and if you did not spot the easter-egg in this cell, you may want to look at the markdown source (^-alt-E).</p>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1619465964521_216733042",
      "id": "paragraph_1589448209034_2145197905",
      "dateCreated": "2021-04-26T19:39:24+0000",
      "status": "READY",
      "focus": true,
      "$$hashKey": "object:5401"
    },
    {
      "text": "%md\n## Before we start\n\nWe can continue our work in the `hey-spark` container, but need to make one modification to the Hadoop setup (even if we have not been using HDFS, Spark configuration is still affected by settings in the Hadoop configuration). Run the following (shell) cell __before you run any Spark commands__ in this notebook (or, restart the interpreter afterwards). _If the `memoryQuery` cell below throws access denied errors, this is the reason._",
      "user": "anonymous",
      "dateUpdated": "2021-04-26T19:39:24+0000",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9,
        "editorHide": true,
        "results": {},
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h2>Before we start</h2>\n<p>We can continue our work in the <code>hey-spark</code> container, but need to make one modification to the Hadoop setup (even if we have not been using HDFS, Spark configuration is still affected by settings in the Hadoop configuration). Run the following (shell) cell <strong>before you run any Spark commands</strong> in this notebook (or, restart the interpreter afterwards). <em>If the <code>memoryQuery</code> cell below throws access denied errors, this is the reason.</em></p>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1619465964521_2011575146",
      "id": "paragraph_1618948191924_2143437076",
      "dateCreated": "2021-04-26T19:39:24+0000",
      "status": "READY",
      "$$hashKey": "object:5402"
    },
    {
      "text": "%sh\nsed -i -e 's|hdfs://localhost:9000|file:///tmp|' /opt/hadoop/etc/hadoop/core-site.xml",
      "user": "anonymous",
      "dateUpdated": "2021-05-10T17:53:22+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "sh",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/sh",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1619465964521_1759856385",
      "id": "paragraph_1618948476848_886180246",
      "dateCreated": "2021-04-26T19:39:24+0000",
      "dateStarted": "2021-05-10T17:53:22+0000",
      "dateFinished": "2021-05-10T17:53:24+0000",
      "status": "FINISHED",
      "$$hashKey": "object:5403"
    },
    {
      "text": "%md\n_One more thing, **read carefully**:_\n\nStream processing is non-trivial, especially because you may be restarting streams etc. more than usual, during the learning process when code did not work out, etc. Spark Structured Streaming has matured quite a bit, but we use everything in non-production setting on a simulated cluster, which may still trigger bugs. A common consequence is the Docker container running out of memory, or the Zeppelin notebook seemingly \"getting stuck\". If that happens, it usually helps to try to stop the container, and then start it again; you may also have to remove state in the form of data directories or checkpoint directories. \n\nThis procedure tends to resolve inexplicable problems I encountered while developing this new lab session (say in 99% of the cases). You find a few commands that may help resolve problems at the very bottom of this Zeppelin notebook!",
      "user": "anonymous",
      "dateUpdated": "2021-04-26T19:39:24+0000",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9,
        "editorHide": true,
        "results": {},
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<p><em>One more thing, <strong>read carefully</strong>:</em></p>\n<p>Stream processing is non-trivial, especially because you may be restarting streams etc. more than usual, during the learning process when code did not work out, etc. Spark Structured Streaming has matured quite a bit, but we use everything in non-production setting on a simulated cluster, which may still trigger bugs. A common consequence is the Docker container running out of memory, or the Zeppelin notebook seemingly &ldquo;getting stuck&rdquo;. If that happens, it usually helps to try to stop the container, and then start it again; you may also have to remove state in the form of data directories or checkpoint directories.</p>\n<p>This procedure tends to resolve inexplicable problems I encountered while developing this new lab session (say in 99% of the cases). You find a few commands that may help resolve problems at the very bottom of this Zeppelin notebook!</p>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1619465964521_393718702",
      "id": "paragraph_1618949536003_1123235532",
      "dateCreated": "2021-04-26T19:39:24+0000",
      "status": "READY",
      "$$hashKey": "object:5404"
    },
    {
      "text": "%md\n## Input Stream\n\nOké, let's start! For the sake of the assignment, you will start a simulation that generates a stream of events _inside_ the course container. This is a simplification (on a simulated cluster, really); in reality you'd be reading your data from a Kafka input stream over an internet connection to a remote server.\n\nThe data generator is a python program that writes [RuneScape](https://runescape.com)-\"like\" output to port 9999.\n\nUse Docker commands to copy the program to the container (only once) and a shell command to actually start the stream (every time you restart the container):\n\n    docker cp stream.py hey-spark:/\n    docker exec hey-spark sh -c \"python3 stream.py &\"\n\nIf you're unsure what's happening, you can inspect the processes running inside the container using `ps -ef` and the output of the python program written to port 9999 using utility `nc`:\n\n    docker exec hey-spark sh -c \"ps -ef\"\n    docker exec hey-spark sh -c \"nc localhost 9999\"\n\nFinally, we are ready to use Spark Structured Streaming to process the stream into a dashboard.",
      "user": "anonymous",
      "dateUpdated": "2021-04-26T19:39:24+0000",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9,
        "editorHide": true,
        "results": {},
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h2>Input Stream</h2>\n<p>Oké, let&rsquo;s start! For the sake of the assignment, you will start a simulation that generates a stream of events <em>inside</em> the course container. This is a simplification (on a simulated cluster, really); in reality you&rsquo;d be reading your data from a Kafka input stream over an internet connection to a remote server.</p>\n<p>The data generator is a python program that writes <a href=\"https://runescape.com\">RuneScape</a>-&ldquo;like&rdquo; output to port 9999.</p>\n<p>Use Docker commands to copy the program to the container (only once) and a shell command to actually start the stream (every time you restart the container):</p>\n<pre><code>docker cp stream.py hey-spark:/\ndocker exec hey-spark sh -c &quot;python3 stream.py &amp;&quot;\n</code></pre>\n<p>If you&rsquo;re unsure what&rsquo;s happening, you can inspect the processes running inside the container using <code>ps -ef</code> and the output of the python program written to port 9999 using utility <code>nc</code>:</p>\n<pre><code>docker exec hey-spark sh -c &quot;ps -ef&quot;\ndocker exec hey-spark sh -c &quot;nc localhost 9999&quot;\n</code></pre>\n<p>Finally, we are ready to use Spark Structured Streaming to process the stream into a dashboard.</p>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1619465964522_1507586128",
      "id": "paragraph_1590346737403_-1899530057",
      "dateCreated": "2021-04-26T19:39:24+0000",
      "status": "READY",
      "$$hashKey": "object:5405"
    },
    {
      "text": "%md\n## Preliminaries\nStart with a few imports:",
      "user": "anonymous",
      "dateUpdated": "2021-04-26T19:39:24+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "text",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/text",
        "fontSize": 9,
        "editorHide": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h2>Preliminaries</h2>\n<p>Start with a few imports:</p>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1619465964522_471074856",
      "id": "paragraph_1590373097354_-1345560625",
      "dateCreated": "2021-04-26T19:39:24+0000",
      "status": "READY",
      "$$hashKey": "object:5406"
    },
    {
      "text": "// A few imports we need later on:\nimport spark.implicits._\nimport org.apache.spark.sql.streaming.Trigger",
      "user": "anonymous",
      "dateUpdated": "2021-05-10T17:53:28+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "import spark.implicits._\nimport org.apache.spark.sql.streaming.Trigger\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1619465964522_1657356325",
      "id": "paragraph_1590373001011_1107059707",
      "dateCreated": "2021-04-26T19:39:24+0000",
      "dateStarted": "2021-05-10T17:53:28+0000",
      "dateFinished": "2021-05-10T17:53:29+0000",
      "status": "FINISHED",
      "$$hashKey": "object:5407"
    },
    {
      "text": "%md\nCreate a dataframe tied to the TCP/IP stream on localhost port 9999 using the `readStream` operation:",
      "user": "anonymous",
      "dateUpdated": "2021-04-26T19:39:24+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "text",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/text",
        "fontSize": 9,
        "editorHide": true,
        "results": {},
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<p>Create a dataframe tied to the TCP/IP stream on localhost port 9999 using the <code>readStream</code> operation:</p>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1619465964522_1957204630",
      "id": "paragraph_1589448302509_1503843353",
      "dateCreated": "2021-04-26T19:39:24+0000",
      "status": "READY",
      "$$hashKey": "object:5408"
    },
    {
      "text": "val socketDF = spark.readStream\n  .format(\"socket\")\n  .option(\"host\", \"0.0.0.0\")\n  .option(\"port\", 9999)\n  .load()",
      "user": "anonymous",
      "dateUpdated": "2021-05-10T17:53:30+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u001b[1m\u001b[34msocketDF\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.DataFrame\u001b[0m = [value: string]\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1619465964522_644975318",
      "id": "paragraph_1589445007354_1672367834",
      "dateCreated": "2021-04-26T19:39:24+0000",
      "dateStarted": "2021-05-10T17:53:30+0000",
      "dateFinished": "2021-05-10T17:53:31+0000",
      "status": "FINISHED",
      "$$hashKey": "object:5409"
    },
    {
      "text": "%md\nWhile the result looks like an ordinary DataFrame at first sight, it identifies itself as a _Streaming Dataframe_ when you check:",
      "user": "anonymous",
      "dateUpdated": "2021-04-26T19:39:24+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "text",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/text",
        "fontSize": 9,
        "editorHide": true,
        "results": {},
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<p>While the result looks like an ordinary DataFrame at first sight, it identifies itself as a <em>Streaming Dataframe</em> when you check:</p>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1619465964522_1480452917",
      "id": "paragraph_1589968457518_487118191",
      "dateCreated": "2021-04-26T19:39:24+0000",
      "status": "READY",
      "$$hashKey": "object:5410"
    },
    {
      "text": "socketDF.isStreaming",
      "user": "anonymous",
      "dateUpdated": "2021-05-10T17:53:33+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u001b[1m\u001b[34mres4\u001b[0m: \u001b[1m\u001b[32mBoolean\u001b[0m = true\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1619465964522_2141812919",
      "id": "paragraph_1589449282216_1888903424",
      "dateCreated": "2021-04-26T19:39:24+0000",
      "dateStarted": "2021-05-10T17:53:33+0000",
      "dateFinished": "2021-05-10T17:53:34+0000",
      "status": "FINISHED",
      "$$hashKey": "object:5411"
    },
    {
      "text": "%md\n## In-memory stream processing\nKeep in mind that Spark has a lazy execution paradigm; nothing has actually happened this far.\n\nLet's move on and write a sample of data from the TCP/IP connection into an in-memory dataframe for further analysis.",
      "user": "anonymous",
      "dateUpdated": "2021-04-26T19:39:24+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "text",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/text",
        "fontSize": 9,
        "editorHide": true,
        "results": {},
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h2>In-memory stream processing</h2>\n<p>Keep in mind that Spark has a lazy execution paradigm; nothing has actually happened this far.</p>\n<p>Let&rsquo;s move on and write a sample of data from the TCP/IP connection into an in-memory dataframe for further analysis.</p>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1619465964522_719028045",
      "id": "paragraph_1590346108921_-205119332",
      "dateCreated": "2021-04-26T19:39:24+0000",
      "status": "READY",
      "$$hashKey": "object:5412"
    },
    {
      "text": "// Setup streamreader\nval streamWriterMem = socketDF\n  .writeStream\n  .outputMode(\"append\")\n  .format(\"memory\")",
      "user": "anonymous",
      "dateUpdated": "2021-05-10T17:53:36+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "editorHide": false,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u001b[1m\u001b[34mstreamWriterMem\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.streaming.DataStreamWriter[org.apache.spark.sql.Row]\u001b[0m = org.apache.spark.sql.streaming.DataStreamWriter@54c276bc\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1619465964522_1457006960",
      "id": "paragraph_1589445076331_1204507247",
      "dateCreated": "2021-04-26T19:39:24+0000",
      "dateStarted": "2021-05-10T17:53:36+0000",
      "dateFinished": "2021-05-10T17:53:36+0000",
      "status": "FINISHED",
      "$$hashKey": "object:5413"
    },
    {
      "text": "// Start streaming!\nval memoryQuery = streamWriterMem  \n  .queryName(\"memoryDF\")\n  .start()\n\n// Run for 1 second...\nmemoryQuery\n  .awaitTermination(1000)\n  \n// ... and stop the query, to avoid filling up memory:\nmemoryQuery\n  .stop()",
      "user": "anonymous",
      "dateUpdated": "2021-05-10T17:53:38+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "editorHide": false,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u001b[1m\u001b[34mmemoryQuery\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.streaming.StreamingQuery\u001b[0m = org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@4619426d\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1619465964522_242995207",
      "id": "paragraph_1590347143256_1052199518",
      "dateCreated": "2021-04-26T19:39:24+0000",
      "dateStarted": "2021-05-10T17:53:38+0000",
      "dateFinished": "2021-05-10T17:53:40+0000",
      "status": "FINISHED",
      "$$hashKey": "object:5414"
    },
    {
      "text": "%md\nThe `memoryQuery` is a `StreamingQuery`, which reads data from a TCP socket, and splits it into individual lines based on newlines.\n\nSince we are streaming into memory, we need to be careful not to overflow it. Streaming for only 1 second to get a feel for the data is a safe bet. If no data is arriving, we retry with a longer window, say, 5 seconds, _etc._\n\nThe query is tied to a Dataframe named `memoryDF` that we can analyze using SQL:",
      "user": "anonymous",
      "dateUpdated": "2021-04-26T19:39:24+0000",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9,
        "editorHide": true,
        "results": {},
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<p>The <code>memoryQuery</code> is a <code>StreamingQuery</code>, which reads data from a TCP socket, and splits it into individual lines based on newlines.</p>\n<p>Since we are streaming into memory, we need to be careful not to overflow it. Streaming for only 1 second to get a feel for the data is a safe bet. If no data is arriving, we retry with a longer window, say, 5 seconds, <em>etc.</em></p>\n<p>The query is tied to a Dataframe named <code>memoryDF</code> that we can analyze using SQL:</p>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1619465964522_437605979",
      "id": "paragraph_1589452113213_-1020765739",
      "dateCreated": "2021-04-26T19:39:24+0000",
      "status": "READY",
      "$$hashKey": "object:5415"
    },
    {
      "text": "%sql\n-- Query the top 10 rows\nselect * from memoryDF LIMIT 10",
      "user": "anonymous",
      "dateUpdated": "2021-05-10T17:53:42+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "sql",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/sql",
        "fontSize": 9,
        "editorHide": false,
        "results": {
          "0": {
            "graph": {
              "mode": "table",
              "height": 300,
              "optionOpen": false,
              "setting": {
                "table": {
                  "tableGridState": {
                    "columns": [
                      {
                        "name": "value0",
                        "visible": true,
                        "width": "*",
                        "sort": {},
                        "filters": [
                          {}
                        ],
                        "pinned": ""
                      }
                    ],
                    "scrollFocus": {},
                    "selection": [],
                    "grouping": {
                      "grouping": [],
                      "aggregations": [],
                      "rowExpandedStates": {}
                    },
                    "treeView": {},
                    "pagination": {
                      "paginationCurrentPage": 1,
                      "paginationPageSize": 250
                    }
                  },
                  "tableColumnTypeState": {
                    "names": {
                      "value": "string"
                    },
                    "updated": false
                  },
                  "tableOptionSpecHash": "[{\"name\":\"useFilter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable filter for columns\"},{\"name\":\"showPagination\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable pagination for better navigation\"},{\"name\":\"showAggregationFooter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable a footer for displaying aggregated values\"}]",
                  "tableOptionValue": {
                    "useFilter": false,
                    "showPagination": false,
                    "showAggregationFooter": false
                  },
                  "updated": false,
                  "initialized": false
                },
                "multiBarChart": {
                  "rotate": {
                    "degree": "-45"
                  },
                  "xLabelStatus": "default"
                },
                "stackedAreaChart": {
                  "rotate": {
                    "degree": "-45"
                  },
                  "xLabelStatus": "default"
                },
                "lineChart": {
                  "rotate": {
                    "degree": "-45"
                  },
                  "xLabelStatus": "default"
                }
              },
              "commonSetting": {},
              "keys": [
                {
                  "name": "value",
                  "index": 0,
                  "aggr": "sum"
                }
              ],
              "groups": [],
              "values": []
            },
            "helium": {}
          }
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TABLE",
            "data": "value\nIron Mace was sold for 1510gp\nMithril Warhammer was sold for 6955gp\nSteel Halberd was sold for 4418gp\nMithril Warhammer was sold for 7008gp\nIron Mace was sold for 1500gp\nMithril Two-handed sword was sold for 8242gp\nBlack Claw was sold for 4543gp\nIron Two-handed sword was sold for 5328gp\nIron Claw was sold for 2307gp\nDragon Longsword was sold for 63019gp\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1619465964522_1825175548",
      "id": "paragraph_1590347520347_1700196331",
      "dateCreated": "2021-04-26T19:39:24+0000",
      "dateStarted": "2021-05-10T17:53:42+0000",
      "dateFinished": "2021-05-10T17:53:42+0000",
      "status": "FINISHED",
      "$$hashKey": "object:5416"
    },
    {
      "text": "%md\nLet's see how many rows we received in total while we were collecting data; and, notice the diffence in calling SQL queries from the Spark SQL interpreter (above) or from the Spark interpreter (below).",
      "user": "anonymous",
      "dateUpdated": "2021-04-26T19:39:24+0000",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9,
        "editorHide": true,
        "results": {},
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<p>Let&rsquo;s see how many rows we received in total while we were collecting data; and, notice the diffence in calling SQL queries from the Spark SQL interpreter (above) or from the Spark interpreter (below).</p>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1619465964522_1651927799",
      "id": "paragraph_1589451920089_-2103181667",
      "dateCreated": "2021-04-26T19:39:24+0000",
      "status": "READY",
      "$$hashKey": "object:5417"
    },
    {
      "text": "spark.sql(\"select count(*) from memoryDF\").show()",
      "user": "anonymous",
      "dateUpdated": "2021-05-10T17:53:52+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "+--------+\n|count(1)|\n+--------+\n|      32|\n+--------+\n\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://277fbf090e68:4040/jobs/job?id=5",
              "$$hashKey": "object:11851"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1619465964522_1544209547",
      "id": "paragraph_1589451990777_1216309247",
      "dateCreated": "2021-04-26T19:39:24+0000",
      "dateStarted": "2021-05-10T17:53:52+0000",
      "dateFinished": "2021-05-10T17:53:53+0000",
      "status": "FINISHED",
      "$$hashKey": "object:5418"
    },
    {
      "text": "%md\nYou can repeat this multiple times, every time with (slightly) different results in sample count, and different output for the `.show()` command. \n\nFeel free to vary the time you read from the stream, or execute different SQL commands. Remember that all processing is in-memory, so take care not to collect too much data though. On a real cluster, that would be less problematic, but your resources are limited to those of your own machine (or the ones in Huygens).",
      "user": "anonymous",
      "dateUpdated": "2021-04-26T19:39:24+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "text",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/text",
        "fontSize": 9,
        "editorHide": true,
        "results": {},
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<p>You can repeat this multiple times, every time with (slightly) different results in sample count, and different output for the <code>.show()</code> command.</p>\n<p>Feel free to vary the time you read from the stream, or execute different SQL commands. Remember that all processing is in-memory, so take care not to collect too much data though. On a real cluster, that would be less problematic, but your resources are limited to those of your own machine (or the ones in Huygens).</p>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1619465964522_1988976050",
      "id": "paragraph_1590349469263_-981939215",
      "dateCreated": "2021-04-26T19:39:24+0000",
      "status": "READY",
      "$$hashKey": "object:5419"
    },
    {
      "text": "%md\n## Parsing the input stream\n\nSo far, we have simply copied data from the input stream into a Dataframe in memory. Now that we know the structure of the stream messages, we should transform the data from `String` into a structure that can be processed in a more meaningful way. Use a regular expression on the data read from the stream before you write it out to memory. \n\nThe hints in the comments are meant to help you get started with the regular expression you need (add another extraction) and construction of a SQL query with multiple uses of the same regex, using Scala's [string interpolation](https://docs.scala-lang.org/overviews/core/string-interpolation.html).\n\n_Warning: The `case class` definition has to be executed as a separate cell, and must have been executed before running the cell where you need it defined as a type. You'll see errors complaining about undefined `Encoders` otherwise._",
      "user": "anonymous",
      "dateUpdated": "2021-04-26T19:39:24+0000",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9,
        "editorHide": true,
        "results": {},
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h2>Parsing the input stream</h2>\n<p>So far, we have simply copied data from the input stream into a Dataframe in memory. Now that we know the structure of the stream messages, we should transform the data from <code>String</code> into a structure that can be processed in a more meaningful way. Use a regular expression on the data read from the stream before you write it out to memory.</p>\n<p>The hints in the comments are meant to help you get started with the regular expression you need (add another extraction) and construction of a SQL query with multiple uses of the same regex, using Scala&rsquo;s <a href=\"https://docs.scala-lang.org/overviews/core/string-interpolation.html\">string interpolation</a>.</p>\n<p><em>Warning: The <code>case class</code> definition has to be executed as a separate cell, and must have been executed before running the cell where you need it defined as a type. You&rsquo;ll see errors complaining about undefined <code>Encoders</code> otherwise.</em></p>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1619465964522_608690111",
      "id": "paragraph_1590352535980_-430940734",
      "dateCreated": "2021-04-26T19:39:24+0000",
      "status": "READY",
      "$$hashKey": "object:5420"
    },
    {
      "text": "// Hint: modify the type definitions to produce RuneData as triples <material, tpe, price>\n\ncase class RuneData(material: String, tpe: String, price: Integer)",
      "user": "anonymous",
      "dateUpdated": "2021-05-10T18:00:55+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "defined class RuneData\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1619465964523_178980264",
      "id": "paragraph_1618938521493_1949979954",
      "dateCreated": "2021-04-26T19:39:24+0000",
      "dateStarted": "2021-05-10T18:00:55+0000",
      "dateFinished": "2021-05-10T18:00:56+0000",
      "status": "FINISHED",
      "$$hashKey": "object:5421"
    },
    {
      "text": "// Hint: modify the regular expression to parse the strings taken from the stream into triples\nval myregex = \"\\\"^([A-Z].+) ([A-Z].+) was sold for (\\\\\\\\d+)\\\"\"\nval q = f\"select regexp_extract(value, $myregex%s, 1) as material, regexp_extract(value, $myregex%s, 2) as tpe, cast(regexp_extract(value, $myregex%s, 3) as Integer) as price from memoryDF\"\nspark.sql(q).as[RuneData].show(10, false)",
      "user": "anonymous",
      "dateUpdated": "2021-05-10T18:04:02+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "results": {
          "0": {
            "graph": {
              "mode": "table",
              "height": 334,
              "optionOpen": false
            }
          }
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "+--------+----------------+-----+\n|material|tpe             |price|\n+--------+----------------+-----+\n|Iron    |Mace            |1510 |\n|Mithril |Warhammer       |6955 |\n|Steel   |Halberd         |4418 |\n|Mithril |Warhammer       |7008 |\n|Iron    |Mace            |1500 |\n|Mithril |Two-handed sword|8242 |\n|Black   |Claw            |4543 |\n|Iron    |Two-handed sword|5328 |\n|Iron    |Claw            |2307 |\n|Dragon  |Longsword       |63019|\n+--------+----------------+-----+\nonly showing top 10 rows\n\n\u001b[1m\u001b[34mmyregex\u001b[0m: \u001b[1m\u001b[32mString\u001b[0m = \"^([A-Z].+) ([A-Z].+) was sold for (\\\\d+)\"\n\u001b[1m\u001b[34mq\u001b[0m: \u001b[1m\u001b[32mString\u001b[0m = select regexp_extract(value, \"^([A-Z].+) ([A-Z].+) was sold for (\\\\d+)\", 1) as material, regexp_extract(value, \"^([A-Z].+) ([A-Z].+) was sold for (\\\\d+)\", 2) as tpe, cast(regexp_extract(value, \"^([A-Z].+) ([A-Z].+) was sold for (\\\\d+)\", 3) as Integer) as price from memoryDF\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://277fbf090e68:4040/jobs/job?id=128",
              "$$hashKey": "object:12766"
            },
            {
              "jobUrl": "http://277fbf090e68:4040/jobs/job?id=129",
              "$$hashKey": "object:12767"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1619465964523_2044579107",
      "id": "paragraph_1590352674285_961143901",
      "dateCreated": "2021-04-26T19:39:24+0000",
      "dateStarted": "2021-05-10T18:04:02+0000",
      "dateFinished": "2021-05-10T18:04:03+0000",
      "status": "FINISHED",
      "$$hashKey": "object:5422"
    },
    {
      "text": "%md\n## Stream Processing\n\nSo far, we took samples from a stream and prepared our code for parsing that stream. Let us now switch to continuous stream processing;\nfirst using console output for debugging, and subsequently writing the stream query output to disk.\n\n### Console output\n\nFinally, we will see stream-based processing in action. Let us set an update interval of 5 seconds.",
      "user": "anonymous",
      "dateUpdated": "2021-04-26T19:39:24+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "text",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/text",
        "fontSize": 9,
        "editorHide": true,
        "results": {},
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h2>Stream Processing</h2>\n<p>So far, we took samples from a stream and prepared our code for parsing that stream. Let us now switch to continuous stream processing;<br />\nfirst using console output for debugging, and subsequently writing the stream query output to disk.</p>\n<h3>Console output</h3>\n<p>Finally, we will see stream-based processing in action. Let us set an update interval of 5 seconds.</p>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1619465964523_1210634030",
      "id": "paragraph_1590365012512_-2000840488",
      "dateCreated": "2021-04-26T19:39:24+0000",
      "status": "READY",
      "$$hashKey": "object:5423"
    },
    {
      "text": "// Create and start a streaming query on the same TCP/IP input stream\nval consoleQuery = socketDF\n  .writeStream\n  .outputMode(\"append\")\n  .format(\"console\")\n  .start()",
      "user": "anonymous",
      "dateUpdated": "2021-05-10T18:04:08+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1619465964523_1850610312",
      "id": "paragraph_1590364433946_-196295345",
      "dateCreated": "2021-04-26T19:39:24+0000",
      "status": "FINISHED",
      "$$hashKey": "object:5424",
      "dateFinished": "2021-05-10T18:04:08+0000",
      "dateStarted": "2021-05-10T18:04:08+0000",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u001b[1m\u001b[34mconsoleQuery\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.streaming.StreamingQuery\u001b[0m = org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@546d3093\n"
          }
        ]
      }
    },
    {
      "text": "%md\nIssue the following cell a few times before stopping this trivial query.",
      "user": "anonymous",
      "dateUpdated": "2021-04-26T19:39:24+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "text",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/text",
        "fontSize": 9,
        "editorHide": true,
        "results": {},
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<p>Issue the following cell a few times before stopping this trivial query.</p>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1619465964523_813254465",
      "id": "paragraph_1590365218580_651540030",
      "dateCreated": "2021-04-26T19:39:24+0000",
      "status": "READY",
      "$$hashKey": "object:5425"
    },
    {
      "text": "spark.streams.active",
      "user": "anonymous",
      "dateUpdated": "2021-05-10T18:04:14+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1619465964523_884468812",
      "id": "paragraph_1590365220818_1028967847",
      "dateCreated": "2021-04-26T19:39:24+0000",
      "status": "FINISHED",
      "$$hashKey": "object:5426",
      "dateFinished": "2021-05-10T18:04:14+0000",
      "dateStarted": "2021-05-10T18:04:14+0000",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "-------------------------------------------\nBatch: 39\n-------------------------------------------\n+--------------------+\n|               value|\n+--------------------+\n|Steel Hatchet was...|\n|Rune Claw was sol...|\n|Rune Longsword wa...|\n|Dragon Dagger was...|\n|Rune Mace was sol...|\n|Dragon Dagger was...|\n|Dragon Mace was s...|\n|White Battleaxe w...|\n|Black Longsword w...|\n|Adamant Scimitar ...|\n|Dragon Hasta was ...|\n+--------------------+\n\n-------------------------------------------\nBatch: 40\n-------------------------------------------\n+--------------------+\n|               value|\n+--------------------+\n|Adamant Longsword...|\n|Steel Dagger was ...|\n|White Claw was so...|\n|Black Battleaxe w...|\n|White Battleaxe w...|\n|Iron Mace was sol...|\n|Adamant Longsword...|\n|Steel Scimitar wa...|\n|Bronze Halberd wa...|\n|Bronze Spear was ...|\n|Rune Hasta was so...|\n|Steel Longsword w...|\n|Rune Longsword wa...|\n|Adamant Sword was...|\n|Mithril Battleaxe...|\n|Iron Warhammer wa...|\n|White Longsword w...|\n|Bronze Hatchet wa...|\n|Black Mace was so...|\n|Iron Battleaxe wa...|\n+--------------------+\nonly showing top 20 rows\n\n\u001b[1m\u001b[34mres21\u001b[0m: \u001b[1m\u001b[32mArray[org.apache.spark.sql.streaming.StreamingQuery]\u001b[0m = Array(org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@546d3093)\n"
          }
        ]
      }
    },
    {
      "text": "%md\nStop the query when you have seen enough.",
      "user": "anonymous",
      "dateUpdated": "2021-04-26T19:39:24+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "text",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/text",
        "fontSize": 9,
        "editorHide": true,
        "results": {},
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<p>Stop the query when you have seen enough.</p>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1619465964523_2094011542",
      "id": "paragraph_1590365279388_-363941900",
      "dateCreated": "2021-04-26T19:39:24+0000",
      "status": "READY",
      "$$hashKey": "object:5427"
    },
    {
      "text": "consoleQuery.stop()",
      "user": "anonymous",
      "dateUpdated": "2021-05-10T18:04:17+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1619465964523_372410269",
      "id": "paragraph_1590365223536_1133035361",
      "dateCreated": "2021-04-26T19:39:24+0000",
      "status": "FINISHED",
      "$$hashKey": "object:5428",
      "dateFinished": "2021-05-10T18:04:17+0000",
      "dateStarted": "2021-05-10T18:04:17+0000",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "-------------------------------------------\nBatch: 58\n-------------------------------------------\n+--------------------+\n|               value|\n+--------------------+\n|Rune Two-handed s...|\n|Mithril Hatchet w...|\n|Mithril Battleaxe...|\n|Black Sword was s...|\n|Bronze Two-handed...|\n|Steel Hasta was s...|\n|Black Sword was s...|\n|Black Dagger was ...|\n|Adamant Claw was ...|\n|Bronze Two-handed...|\n|Rune Claw was sol...|\n+--------------------+\n\n-------------------------------------------\nBatch: 59\n-------------------------------------------\n+--------------------+\n|               value|\n+--------------------+\n|Black Claw was so...|\n|Mithril Warhammer...|\n|Steel Spear was s...|\n|Dragon Claw was s...|\n|Dragon Hasta was ...|\n|White Hasta was s...|\n|Black Hasta was s...|\n|Black Warhammer w...|\n|Bronze Sword was ...|\n|Steel Spear was s...|\n|Adamant Warhammer...|\n|Adamant Warhammer...|\n|Bronze Sword was ...|\n|Dragon Spear was ...|\n|Dragon Scimitar w...|\n|Mithril Battleaxe...|\n|Adamant Spear was...|\n|Black Sword was s...|\n|White Claw was so...|\n|White Claw was so...|\n+--------------------+\nonly showing top 20 rows\n\n"
          }
        ]
      }
    },
    {
      "text": "%md\n## Structured console output\nNow it is time to apply our previous code to parse the input stream into `RuneData`.",
      "user": "anonymous",
      "dateUpdated": "2021-04-26T19:39:24+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "text",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/text",
        "fontSize": 9,
        "editorHide": true,
        "results": {},
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h2>Structured console output</h2>\n<p>Now it is time to apply our previous code to parse the input stream into <code>RuneData</code>.</p>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1619465964523_1661869768",
      "id": "paragraph_1590365383386_-132734152",
      "dateCreated": "2021-04-26T19:39:24+0000",
      "status": "READY",
      "$$hashKey": "object:5429"
    },
    {
      "text": "socketDF.createOrReplaceTempView(\"runeUpdatesDF\")",
      "user": "anonymous",
      "dateUpdated": "2021-05-10T18:04:24+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1619465964523_779095312",
      "id": "paragraph_1590366117502_-920559067",
      "dateCreated": "2021-04-26T19:39:24+0000",
      "status": "FINISHED",
      "$$hashKey": "object:5430",
      "dateFinished": "2021-05-10T18:04:25+0000",
      "dateStarted": "2021-05-10T18:04:24+0000"
    },
    {
      "text": "// Define if necessary, in separate cell (admittedly, this is somewhat confusing)\ncase class RuneData(material: String, tpe: String, price: Integer)",
      "user": "anonymous",
      "dateUpdated": "2021-05-10T18:04:27+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1619465964523_232958450",
      "id": "paragraph_1618938800319_1888427924",
      "dateCreated": "2021-04-26T19:39:24+0000",
      "status": "FINISHED",
      "$$hashKey": "object:5431",
      "dateFinished": "2021-05-10T18:04:27+0000",
      "dateStarted": "2021-05-10T18:04:27+0000",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "defined class RuneData\n"
          }
        ]
      }
    },
    {
      "text": "// Use your solution from above to create the runes streaming dataframe you will need below:\n\nval myregex = \"\\\"^([A-Z].+) ([A-Z].+) was sold for (\\\\\\\\d+)\\\"\"\nval q = f\"select regexp_extract(value, $myregex%s, 1) as material, regexp_extract(value, $myregex%s, 2) as tpe, cast(regexp_extract(value, $myregex%s, 3) as Integer) as price from runeUpdatesDF\"\n\nval runes = spark.sql(q).as[RuneData]",
      "user": "anonymous",
      "dateUpdated": "2021-05-10T18:30:30+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1619465964523_537242382",
      "id": "paragraph_1590363637302_1990183194",
      "dateCreated": "2021-04-26T19:39:24+0000",
      "status": "FINISHED",
      "$$hashKey": "object:5432",
      "dateFinished": "2021-05-10T18:30:31+0000",
      "dateStarted": "2021-05-10T18:30:30+0000",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u001b[1m\u001b[34mmyregex\u001b[0m: \u001b[1m\u001b[32mString\u001b[0m = \"^([A-Z].+) ([A-Z].+) was sold for (\\\\d+)\"\n\u001b[1m\u001b[34mq\u001b[0m: \u001b[1m\u001b[32mString\u001b[0m = select regexp_extract(value, \"^([A-Z].+) ([A-Z].+) was sold for (\\\\d+)\", 1) as material, regexp_extract(value, \"^([A-Z].+) ([A-Z].+) was sold for (\\\\d+)\", 2) as tpe, cast(regexp_extract(value, \"^([A-Z].+) ([A-Z].+) was sold for (\\\\d+)\", 3) as Integer) as price from runeUpdatesDF\n\u001b[1m\u001b[34mrunes\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.Dataset[RuneData]\u001b[0m = [material: string, tpe: string ... 1 more field]\n"
          }
        ]
      }
    },
    {
      "text": "%md\nAgain, mind the lazy Spark evaluation!\nWe have prepared a streaming query plan `runes` to get our `RuneData` out of the socket in a streaming fashion. \n\nLike above, start a query that streams to console, let it run for a while, and look at the output when you stop the query or investigate state in between:",
      "user": "anonymous",
      "dateUpdated": "2021-04-26T19:39:24+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "text",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/text",
        "fontSize": 9,
        "editorHide": true,
        "results": {},
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<p>Again, mind the lazy Spark evaluation!<br />\nWe have prepared a streaming query plan <code>runes</code> to get our <code>RuneData</code> out of the socket in a streaming fashion.</p>\n<p>Like above, start a query that streams to console, let it run for a while, and look at the output when you stop the query or investigate state in between:</p>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1619465964523_1167209360",
      "id": "paragraph_1590366295075_-1846777262",
      "dateCreated": "2021-04-26T19:39:24+0000",
      "status": "READY",
      "$$hashKey": "object:5433"
    },
    {
      "text": "val rConsoleQuery = runes\n  .writeStream\n  .outputMode(\"append\")\n  .format(\"console\")\n  .start()",
      "user": "anonymous",
      "dateUpdated": "2021-05-10T18:30:36+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1619465964523_1808245478",
      "id": "paragraph_1590366302193_1646148825",
      "dateCreated": "2021-04-26T19:39:24+0000",
      "status": "FINISHED",
      "$$hashKey": "object:5434",
      "dateFinished": "2021-05-10T18:30:36+0000",
      "dateStarted": "2021-05-10T18:30:36+0000",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u001b[1m\u001b[34mrConsoleQuery\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.streaming.StreamingQuery\u001b[0m = org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@216bf67b\n"
          }
        ]
      }
    },
    {
      "text": "spark.streams.active",
      "user": "anonymous",
      "dateUpdated": "2021-05-10T18:30:51+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1619465964523_157570579",
      "id": "paragraph_1590366560008_-96132883",
      "dateCreated": "2021-04-26T19:39:24+0000",
      "status": "FINISHED",
      "$$hashKey": "object:5435",
      "dateFinished": "2021-05-10T18:30:52+0000",
      "dateStarted": "2021-05-10T18:30:51+0000",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "-------------------------------------------\nBatch: 110\n-------------------------------------------\n+--------+----------------+------+\n|material|             tpe| price|\n+--------+----------------+------+\n|    Iron|           Sword|  1224|\n|  Bronze|        Scimitar|  1180|\n| Adamant|            Mace|  3314|\n| Mithril|       Longsword|  4180|\n|    Iron|            Claw|  2424|\n|   Black|           Hasta|  2814|\n|  Dragon|          Dagger| 20258|\n|  Bronze|       Longsword|  1183|\n|    Iron|        Scimitar|  2688|\n|  Bronze|       Warhammer|  1936|\n| Mithril|           Hasta|  2373|\n|  Dragon|Two-handed sword|125854|\n|    Iron|       Battleaxe|  4724|\n|    Iron|          Dagger|   823|\n|  Dragon|        Scimitar| 62892|\n|    Iron|       Battleaxe|  4703|\n+--------+----------------+------+\n\n-------------------------------------------\nBatch: 111\n-------------------------------------------\n\u001b[1m\u001b[34mres27\u001b[0m: \u001b[1m\u001b[32mArray[org.apache.spark.sql.streaming.StreamingQuery]\u001b[0m = Array(org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@216bf67b)\n"
          }
        ]
      }
    },
    {
      "text": "rConsoleQuery.stop()",
      "user": "anonymous",
      "dateUpdated": "2021-05-10T18:30:54+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1619465964523_730870081",
      "id": "paragraph_1590366322745_-432112087",
      "dateCreated": "2021-04-26T19:39:24+0000",
      "status": "FINISHED",
      "$$hashKey": "object:5436",
      "dateFinished": "2021-05-10T18:30:54+0000",
      "dateStarted": "2021-05-10T18:30:54+0000",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "+--------+----------------+-----+\n|material|             tpe|price|\n+--------+----------------+-----+\n|   White|        Scimitar| 3323|\n|  Bronze|       Warhammer| 1924|\n|    Rune|          Dagger| 4566|\n| Mithril|Two-handed sword| 8438|\n|   Steel|           Spear| 1707|\n|  Bronze|          Dagger|  390|\n| Adamant|        Scimitar| 5809|\n|   White|       Longsword| 3368|\n|   Steel|       Longsword| 1706|\n|   White|       Warhammer| 5622|\n|   White|        Scimitar| 3304|\n|   Black|           Hasta| 2644|\n+--------+----------------+-----+\n\n"
          }
        ]
      }
    },
    {
      "text": "%md\n## Writing output to disk\n\nIn a more realistic streaming setup, we would not write our output to the console. \n\nWith Spark Structured Streaming, it is almost trivial to stream the query output straight into the filesystem, into Parquet files ready for further analysis. First, create a directory into which we will write out the data we sample from the stream.",
      "user": "anonymous",
      "dateUpdated": "2021-04-26T19:39:24+0000",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "text",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/text",
        "fontSize": 9,
        "editorHide": true,
        "results": {},
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h2>Writing output to disk</h2>\n<p>In a more realistic streaming setup, we would not write our output to the console.</p>\n<p>With Spark Structured Streaming, it is almost trivial to stream the query output straight into the filesystem, into Parquet files ready for further analysis. First, create a directory into which we will write out the data we sample from the stream.</p>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1619465964523_1972454634",
      "id": "paragraph_1589452249641_515522795",
      "dateCreated": "2021-04-26T19:39:24+0000",
      "status": "READY",
      "$$hashKey": "object:5437"
    },
    {
      "text": "%sh\nmkdir -p /opt/hadoop/share/runedata",
      "user": "anonymous",
      "dateUpdated": "2021-05-10T18:31:01+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "sh",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/sh",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1619465964523_19758938",
      "id": "paragraph_1589449045934_-244136830",
      "dateCreated": "2021-04-26T19:39:24+0000",
      "status": "FINISHED",
      "$$hashKey": "object:5438",
      "dateFinished": "2021-05-10T18:31:01+0000",
      "dateStarted": "2021-05-10T18:31:01+0000",
      "results": {
        "code": "SUCCESS",
        "msg": []
      }
    },
    {
      "text": "%md\nSetup another writer to copy the query output to disk.",
      "user": "anonymous",
      "dateUpdated": "2021-04-26T19:39:24+0000",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "text",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/text",
        "fontSize": 9,
        "editorHide": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<p>Setup another writer to copy the query output to disk.</p>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1619465964523_119316723",
      "id": "paragraph_1589452477092_-586340868",
      "dateCreated": "2021-04-26T19:39:24+0000",
      "status": "READY",
      "$$hashKey": "object:5439"
    },
    {
      "text": "val streamWriterDisk = runes\n  .writeStream\n  .outputMode(\"append\")\n  .option(\"checkpointLocation\", \"file:///tmp/checkpoint\")\n  .trigger(Trigger.ProcessingTime(\"2 seconds\"))",
      "user": "anonymous",
      "dateUpdated": "2021-05-10T18:31:08+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1619465964524_738295316",
      "id": "paragraph_1589451076971_191695654",
      "dateCreated": "2021-04-26T19:39:24+0000",
      "status": "FINISHED",
      "$$hashKey": "object:5440",
      "dateFinished": "2021-05-10T18:31:09+0000",
      "dateStarted": "2021-05-10T18:31:08+0000",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u001b[1m\u001b[34mstreamWriterDisk\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.streaming.DataStreamWriter[RuneData]\u001b[0m = org.apache.spark.sql.streaming.DataStreamWriter@62dd90a8\n"
          }
        ]
      }
    },
    {
      "text": "%md\nReady?\n\nThen start the query!",
      "user": "anonymous",
      "dateUpdated": "2021-04-26T19:39:24+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "text",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/text",
        "fontSize": 9,
        "editorHide": true,
        "results": {},
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<p>Ready?</p>\n<p>Then start the query!</p>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1619465964524_2034913704",
      "id": "paragraph_1590370516708_-1662229510",
      "dateCreated": "2021-04-26T19:39:24+0000",
      "status": "READY",
      "$$hashKey": "object:5441"
    },
    {
      "text": "val stream2disk = streamWriterDisk\n  .start(\"file:///opt/hadoop/share/runedata\")",
      "user": "anonymous",
      "dateUpdated": "2021-05-10T18:31:16+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "editorHide": false,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1619465964524_1952744019",
      "id": "paragraph_1590343545514_1652541639",
      "dateCreated": "2021-04-26T19:39:24+0000",
      "status": "FINISHED",
      "$$hashKey": "object:5442",
      "dateFinished": "2021-05-10T18:31:16+0000",
      "dateStarted": "2021-05-10T18:31:16+0000",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u001b[1m\u001b[34mstream2disk\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.streaming.StreamingQuery\u001b[0m = org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@b8c3e74\n"
          }
        ]
      }
    },
    {
      "text": "%md\nIf all worked out well, the following command lists a running streaming query (in a Scala Array).",
      "user": "anonymous",
      "dateUpdated": "2021-04-26T19:39:24+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "text",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/text",
        "fontSize": 9,
        "editorHide": true,
        "results": {},
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<p>If all worked out well, the following command lists a running streaming query (in a Scala Array).</p>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1619465964524_503380472",
      "id": "paragraph_1590368943647_587099821",
      "dateCreated": "2021-04-26T19:39:24+0000",
      "status": "READY",
      "$$hashKey": "object:5443"
    },
    {
      "text": "spark.streams.active",
      "user": "anonymous",
      "dateUpdated": "2021-05-10T18:31:23+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1619465964524_1781357009",
      "id": "paragraph_1590367515865_536586416",
      "dateCreated": "2021-04-26T19:39:24+0000",
      "status": "FINISHED",
      "$$hashKey": "object:5444",
      "dateFinished": "2021-05-10T18:31:24+0000",
      "dateStarted": "2021-05-10T18:31:23+0000",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u001b[1m\u001b[34mres29\u001b[0m: \u001b[1m\u001b[32mArray[org.apache.spark.sql.streaming.StreamingQuery]\u001b[0m = Array(org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@b8c3e74)\n"
          }
        ]
      }
    },
    {
      "text": "%md\n_No active streams?_\n\nIn case of missing output or other reasons to suspect an error, check the streaming query's exception state as follows:",
      "user": "anonymous",
      "dateUpdated": "2021-04-26T19:39:24+0000",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9,
        "editorHide": true,
        "results": {},
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<p><em>No active streams?</em></p>\n<p>In case of missing output or other reasons to suspect an error, check the streaming query&rsquo;s exception state as follows:</p>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1619465964524_191266109",
      "id": "paragraph_1590368877516_1943440320",
      "dateCreated": "2021-04-26T19:39:24+0000",
      "status": "READY",
      "$$hashKey": "object:5445"
    },
    {
      "text": "stream2disk.exception",
      "user": "anonymous",
      "dateUpdated": "2021-05-10T18:31:31+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1619465964524_1655782414",
      "id": "paragraph_1590368773239_1239609256",
      "dateCreated": "2021-04-26T19:39:24+0000",
      "status": "FINISHED",
      "$$hashKey": "object:5446",
      "dateFinished": "2021-05-10T18:31:31+0000",
      "dateStarted": "2021-05-10T18:31:31+0000",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u001b[1m\u001b[34mres30\u001b[0m: \u001b[1m\u001b[32mOption[org.apache.spark.sql.streaming.StreamingQueryException]\u001b[0m = None\n"
          }
        ]
      }
    },
    {
      "text": "%md\n_Errors?_\n\nQuite likely, the problem can be fixed by cleaning out the checkpointing directory under `/tmp`.",
      "user": "anonymous",
      "dateUpdated": "2021-04-26T19:39:24+0000",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9,
        "editorHide": true,
        "results": {},
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<p><em>Errors?</em></p>\n<p>Quite likely, the problem can be fixed by cleaning out the checkpointing directory under <code>/tmp</code>.</p>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1619465964524_1552149503",
      "id": "paragraph_1618951300870_1448197082",
      "dateCreated": "2021-04-26T19:39:24+0000",
      "status": "READY",
      "$$hashKey": "object:5447"
    },
    {
      "text": "%md\nSlowly but steadily, your disk may fill up:",
      "user": "anonymous",
      "dateUpdated": "2021-04-26T19:39:24+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "text",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/text",
        "fontSize": 9,
        "editorHide": true,
        "results": {},
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<p>Slowly but steadily, your disk may fill up:</p>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1619465964524_330456733",
      "id": "paragraph_1590369939018_46179868",
      "dateCreated": "2021-04-26T19:39:24+0000",
      "status": "READY",
      "$$hashKey": "object:5448"
    },
    {
      "text": "%sh\ndu --si /opt/hadoop/share/runedata",
      "user": "anonymous",
      "dateUpdated": "2021-05-10T18:32:28+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "sh",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/sh",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1619465964524_1396112488",
      "id": "paragraph_1590332129421_792210783",
      "dateCreated": "2021-04-26T19:39:24+0000",
      "status": "FINISHED",
      "$$hashKey": "object:5449",
      "dateFinished": "2021-05-10T18:32:28+0000",
      "dateStarted": "2021-05-10T18:32:28+0000",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "357k\t/opt/hadoop/share/runedata/_spark_metadata\n1.6M\t/opt/hadoop/share/runedata\n"
          }
        ]
      }
    },
    {
      "text": "// Stop the stream after a while;\n// for example, when say when you collected a few megabytes of data:\nstream2disk.stop()",
      "user": "anonymous",
      "dateUpdated": "2021-05-10T18:32:33+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1619465964524_1454728254",
      "id": "paragraph_1590369941493_1881016502",
      "dateCreated": "2021-04-26T19:39:24+0000",
      "status": "FINISHED",
      "$$hashKey": "object:5450",
      "dateFinished": "2021-05-10T18:32:34+0000",
      "dateStarted": "2021-05-10T18:32:33+0000",
      "results": {
        "code": "SUCCESS",
        "msg": []
      }
    },
    {
      "text": "%md\n_Checkpointing:_\n\nCheckpointing is what is needed for fault-tolerance in an operational streaming setting. I'd be happy if you'd dive into it, but it's ok for now to just take that for granted. Roughly, because we defined a trigger on this query, every other second a checkpoint should have been created, and a microbatch written to disk.",
      "user": "anonymous",
      "dateUpdated": "2021-04-26T19:39:24+0000",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9,
        "editorHide": true,
        "results": {},
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<p><em>Checkpointing:</em></p>\n<p>Checkpointing is what is needed for fault-tolerance in an operational streaming setting. I&rsquo;d be happy if you&rsquo;d dive into it, but it&rsquo;s ok for now to just take that for granted. Roughly, because we defined a trigger on this query, every other second a checkpoint should have been created, and a microbatch written to disk.</p>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1619465964524_1443832560",
      "id": "paragraph_1590342468854_82445142",
      "dateCreated": "2021-04-26T19:39:24+0000",
      "status": "READY",
      "$$hashKey": "object:5451"
    },
    {
      "text": "%sh\necho \"Checkpoints: $(eval ls /opt/hadoop/share/runedata | wc -l)\"",
      "user": "anonymous",
      "dateUpdated": "2021-05-10T18:32:53+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "sh",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/sh",
        "fontSize": 9,
        "editorHide": false,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1619465964524_1828871998",
      "id": "paragraph_1589453278436_-2041112869",
      "dateCreated": "2021-04-26T19:39:24+0000",
      "status": "FINISHED",
      "$$hashKey": "object:5452",
      "dateFinished": "2021-05-10T18:32:53+0000",
      "dateStarted": "2021-05-10T18:32:53+0000",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "Checkpoints: 156\n"
          }
        ]
      }
    },
    {
      "text": "%md\n## Working with the data collected\nWe can open the sample that was written to disk for analysis using the Dataframe API like we did in assignment 3B. Consider for example a few aggregate queries to produce average price over all items of a certain material, or the minimum or maximum price paid in an transaction.",
      "user": "anonymous",
      "dateUpdated": "2021-04-26T19:39:24+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "text",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/text",
        "fontSize": 9,
        "editorHide": true,
        "results": {},
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h2>Working with the data collected</h2>\n<p>We can open the sample that was written to disk for analysis using the Dataframe API like we did in assignment 3B. Consider for example a few aggregate queries to produce average price over all items of a certain material, or the minimum or maximum price paid in an transaction.</p>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1619465964524_1120054640",
      "id": "paragraph_1589453362862_-465527225",
      "dateCreated": "2021-04-26T19:39:24+0000",
      "status": "READY",
      "$$hashKey": "object:5453"
    },
    {
      "text": "val runes = spark\n  .read\n  .parquet(\"file:///opt/hadoop/share/runedata/part-*\")\n  .createOrReplaceTempView(\"runes\")",
      "user": "anonymous",
      "dateUpdated": "2021-05-10T18:33:13+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://277fbf090e68:4040/jobs/job?id=361",
              "$$hashKey": "object:13960"
            },
            {
              "jobUrl": "http://277fbf090e68:4040/jobs/job?id=362",
              "$$hashKey": "object:13961"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1619465964524_393824841",
      "id": "paragraph_1589447453688_-947324524",
      "dateCreated": "2021-04-26T19:39:24+0000",
      "status": "FINISHED",
      "$$hashKey": "object:5454",
      "dateFinished": "2021-05-10T18:33:15+0000",
      "dateStarted": "2021-05-10T18:33:13+0000",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u001b[1m\u001b[34mrunes\u001b[0m: \u001b[1m\u001b[32mUnit\u001b[0m = ()\n"
          }
        ]
      }
    },
    {
      "text": "%sql\n-- Average price per material:\nSELECT material, avg(price) FROM runes GROUP BY material",
      "user": "anonymous",
      "dateUpdated": "2021-05-10T18:33:20+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "sql",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/sql",
        "fontSize": 9,
        "results": {
          "0": {
            "graph": {
              "mode": "table",
              "height": 300,
              "optionOpen": false,
              "setting": {
                "table": {
                  "tableGridState": {},
                  "tableColumnTypeState": {
                    "names": {
                      "material": "string",
                      "avg(price)": "string"
                    },
                    "updated": false
                  },
                  "tableOptionSpecHash": "[{\"name\":\"useFilter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable filter for columns\"},{\"name\":\"showPagination\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable pagination for better navigation\"},{\"name\":\"showAggregationFooter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable a footer for displaying aggregated values\"}]",
                  "tableOptionValue": {
                    "useFilter": false,
                    "showPagination": false,
                    "showAggregationFooter": false
                  },
                  "updated": false,
                  "initialized": false
                }
              },
              "commonSetting": {}
            }
          }
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://277fbf090e68:4040/jobs/job?id=363",
              "$$hashKey": "object:14170"
            },
            {
              "jobUrl": "http://277fbf090e68:4040/jobs/job?id=364",
              "$$hashKey": "object:14171"
            },
            {
              "jobUrl": "http://277fbf090e68:4040/jobs/job?id=365",
              "$$hashKey": "object:14172"
            },
            {
              "jobUrl": "http://277fbf090e68:4040/jobs/job?id=366",
              "$$hashKey": "object:14173"
            },
            {
              "jobUrl": "http://277fbf090e68:4040/jobs/job?id=367",
              "$$hashKey": "object:14174"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1619465964524_1757133454",
      "id": "paragraph_1590367616021_552396401",
      "dateCreated": "2021-04-26T19:39:24+0000",
      "status": "FINISHED",
      "$$hashKey": "object:5455",
      "dateFinished": "2021-05-10T18:33:23+0000",
      "dateStarted": "2021-05-10T18:33:20+0000",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TABLE",
            "data": "material\tavg(price)\nSteel\t1864.8861076345431\nMithril\t4617.005875440658\nRune\t14470.09589041096\nIron\t2931.8722891566267\nAdamant\t6630.478421701603\nDragon\t69229.42302878598\nWhite\t3701.726572528883\nBlack\t5635.325259515571\nBronze\t1279.9705549263874\n"
          }
        ]
      }
    },
    {
      "text": "%sql\n-- Price variation per material:\nSELECT material, min(price), max(price) FROM runes GROUP BY material",
      "user": "anonymous",
      "dateUpdated": "2021-05-10T18:33:30+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "sql",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/sql",
        "fontSize": 9,
        "results": {
          "0": {
            "graph": {
              "mode": "multiBarChart",
              "height": 300,
              "optionOpen": false,
              "setting": {
                "table": {
                  "tableGridState": {},
                  "tableColumnTypeState": {
                    "names": {
                      "material": "string",
                      "min(price)": "string",
                      "max(price)": "string"
                    },
                    "updated": false
                  },
                  "tableOptionSpecHash": "[{\"name\":\"useFilter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable filter for columns\"},{\"name\":\"showPagination\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable pagination for better navigation\"},{\"name\":\"showAggregationFooter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable a footer for displaying aggregated values\"}]",
                  "tableOptionValue": {
                    "useFilter": false,
                    "showPagination": false,
                    "showAggregationFooter": false
                  },
                  "updated": false,
                  "initialized": false
                },
                "multiBarChart": {
                  "rotate": {
                    "degree": "-45"
                  },
                  "xLabelStatus": "default",
                  "stacked": false
                }
              },
              "commonSetting": {},
              "keys": [
                {
                  "name": "material",
                  "index": 0,
                  "aggr": "sum"
                }
              ],
              "groups": [],
              "values": [
                {
                  "name": "min(price)",
                  "index": 1,
                  "aggr": "sum"
                },
                {
                  "name": "max(price)",
                  "index": 2,
                  "aggr": "sum"
                }
              ]
            },
            "helium": {}
          }
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://277fbf090e68:4040/jobs/job?id=368",
              "$$hashKey": "object:14766"
            },
            {
              "jobUrl": "http://277fbf090e68:4040/jobs/job?id=369",
              "$$hashKey": "object:14767"
            },
            {
              "jobUrl": "http://277fbf090e68:4040/jobs/job?id=370",
              "$$hashKey": "object:14768"
            },
            {
              "jobUrl": "http://277fbf090e68:4040/jobs/job?id=371",
              "$$hashKey": "object:14769"
            },
            {
              "jobUrl": "http://277fbf090e68:4040/jobs/job?id=372",
              "$$hashKey": "object:14770"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1619465964524_1870257214",
      "id": "paragraph_1590374447162_-1852692101",
      "dateCreated": "2021-04-26T19:39:24+0000",
      "status": "FINISHED",
      "$$hashKey": "object:5456",
      "dateFinished": "2021-05-10T18:33:32+0000",
      "dateStarted": "2021-05-10T18:33:30+0000",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TABLE",
            "data": "material\tmin(price)\tmax(price)\nSteel\t490\t4511\nMithril\t1174\t10803\nRune\t4062\t33762\nIron\t792\t6953\nAdamant\t1761\t15287\nDragon\t20439\t161664\nWhite\t1020\t8697\nBlack\t1551\t13039\nBronze\t315\t3075\n"
          }
        ]
      }
    },
    {
      "text": "%md\n## Next steps\n\nYou have reached the point where the initiative is yours; time for some creativity!\n\nTry to create your own dashboard, that you might have used to take decisions in the original game.\nAt the minimum, write code to answer the following questions:\n\n- How many rune items were sold?\n- How many of each item type was sold?\n- How much gold was spent buying swords?\n\nIn the previous step however, we wrote the individual transactions to disk and then carried out analyses using static dataframes. Can you get (one or more of these) other interesting properties of the stream to be updated per microbatch (instead of being run on the full dataset loaded into the cluster)?\n\nCan you think of related queries, or have a go at an even more advanced report that is continuously updated? What we are after is a real-time agent that would buy and sell on the market and make you rich while you study ;-)\n\nI am looking forward to reading about your results in the A5 blog posts!",
      "user": "anonymous",
      "dateUpdated": "2021-04-26T19:39:24+0000",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9,
        "editorHide": true,
        "results": {},
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h2>Next steps</h2>\n<p>You have reached the point where the initiative is yours; time for some creativity!</p>\n<p>Try to create your own dashboard, that you might have used to take decisions in the original game.<br />\nAt the minimum, write code to answer the following questions:</p>\n<ul>\n<li>How many rune items were sold?</li>\n<li>How many of each item type was sold?</li>\n<li>How much gold was spent buying swords?</li>\n</ul>\n<p>In the previous step however, we wrote the individual transactions to disk and then carried out analyses using static dataframes. Can you get (one or more of these) other interesting properties of the stream to be updated per microbatch (instead of being run on the full dataset loaded into the cluster)?</p>\n<p>Can you think of related queries, or have a go at an even more advanced report that is continuously updated? What we are after is a real-time agent that would buy and sell on the market and make you rich while you study ;-)</p>\n<p>I am looking forward to reading about your results in the A5 blog posts!</p>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1619465964524_976510723",
      "id": "paragraph_1589447952519_-1056716615",
      "dateCreated": "2021-04-26T19:39:24+0000",
      "status": "READY",
      "$$hashKey": "object:5457"
    },
    {
      "text": "// \n// Scala Playground\n//\n\n",
      "user": "anonymous",
      "dateUpdated": "2021-04-26T19:39:24+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1619465964525_306211350",
      "id": "paragraph_1618939047139_1732173218",
      "dateCreated": "2021-04-26T19:39:24+0000",
      "status": "READY",
      "$$hashKey": "object:5458"
    },
    {
      "text": "%sql\nSELECT COUNT(*) AS nrSales FROM runes",
      "user": "anonymous",
      "dateUpdated": "2021-05-10T18:37:57+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "sql",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/sql",
        "fontSize": 9,
        "results": {
          "0": {
            "graph": {
              "mode": "table",
              "height": 300,
              "optionOpen": false,
              "setting": {
                "table": {
                  "tableGridState": {},
                  "tableColumnTypeState": {
                    "names": {
                      "nrSales": "string"
                    },
                    "updated": false
                  },
                  "tableOptionSpecHash": "[{\"name\":\"useFilter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable filter for columns\"},{\"name\":\"showPagination\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable pagination for better navigation\"},{\"name\":\"showAggregationFooter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable a footer for displaying aggregated values\"}]",
                  "tableOptionValue": {
                    "useFilter": false,
                    "showPagination": false,
                    "showAggregationFooter": false
                  },
                  "updated": false,
                  "initialized": false
                }
              },
              "commonSetting": {}
            }
          }
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://277fbf090e68:4040/jobs/job?id=377",
              "$$hashKey": "object:16388"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1619465964525_521938286",
      "id": "paragraph_1618951847459_391331045",
      "dateCreated": "2021-04-26T19:39:24+0000",
      "status": "FINISHED",
      "$$hashKey": "object:5459",
      "dateFinished": "2021-05-10T18:37:57+0000",
      "dateStarted": "2021-05-10T18:37:57+0000",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TABLE",
            "data": "nrSales\n7422\n"
          }
        ]
      }
    },
    {
      "text": "%sql\n\nSELECT  tpe as type, COUNT(*) as count FROM runes GROUP BY tpe",
      "user": "anonymous",
      "dateUpdated": "2021-05-10T18:38:40+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "sql",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/sql",
        "fontSize": 9,
        "results": {
          "0": {
            "graph": {
              "mode": "table",
              "height": 300,
              "optionOpen": false,
              "setting": {
                "table": {
                  "tableGridState": {},
                  "tableColumnTypeState": {
                    "names": {
                      "count": "string",
                      "type": "string"
                    },
                    "updated": false
                  },
                  "tableOptionSpecHash": "[{\"name\":\"useFilter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable filter for columns\"},{\"name\":\"showPagination\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable pagination for better navigation\"},{\"name\":\"showAggregationFooter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable a footer for displaying aggregated values\"}]",
                  "tableOptionValue": {
                    "useFilter": false,
                    "showPagination": false,
                    "showAggregationFooter": false
                  },
                  "updated": false,
                  "initialized": false
                }
              },
              "commonSetting": {}
            }
          }
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://277fbf090e68:4040/jobs/job?id=393",
              "$$hashKey": "object:18934"
            },
            {
              "jobUrl": "http://277fbf090e68:4040/jobs/job?id=394",
              "$$hashKey": "object:18935"
            },
            {
              "jobUrl": "http://277fbf090e68:4040/jobs/job?id=395",
              "$$hashKey": "object:18936"
            },
            {
              "jobUrl": "http://277fbf090e68:4040/jobs/job?id=396",
              "$$hashKey": "object:18937"
            },
            {
              "jobUrl": "http://277fbf090e68:4040/jobs/job?id=397",
              "$$hashKey": "object:18938"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1620671880922_1878421366",
      "id": "paragraph_1620671880922_1878421366",
      "dateCreated": "2021-05-10T18:38:00+0000",
      "status": "FINISHED",
      "focus": true,
      "$$hashKey": "object:16591",
      "dateFinished": "2021-05-10T18:38:41+0000",
      "dateStarted": "2021-05-10T18:38:40+0000",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TABLE",
            "data": "type\tcount\nSword\t588\nHasta\t575\nTwo-handed sword\t588\nHatchet\t565\nDagger\t558\nHalberd\t521\nWarhammer\t545\nSpear\t580\nLongsword\t563\nMace\t562\nClaw\t582\nBattleaxe\t591\nScimitar\t604\n"
          }
        ]
      }
    },
    {
      "text": "%sql\nSELECT SUM(price) as sum FROM runes WHERE LOWER(tpe) LIKE \"%sword%\"",
      "user": "anonymous",
      "dateUpdated": "2021-05-10T18:44:48+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "sql",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/sql",
        "fontSize": 9,
        "results": {
          "0": {
            "graph": {
              "mode": "table",
              "height": 300,
              "optionOpen": false,
              "setting": {
                "table": {
                  "tableGridState": {},
                  "tableColumnTypeState": {
                    "names": {
                      "sum": "string"
                    },
                    "updated": false
                  },
                  "tableOptionSpecHash": "[{\"name\":\"useFilter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable filter for columns\"},{\"name\":\"showPagination\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable pagination for better navigation\"},{\"name\":\"showAggregationFooter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable a footer for displaying aggregated values\"}]",
                  "tableOptionValue": {
                    "useFilter": false,
                    "showPagination": false,
                    "showAggregationFooter": false
                  },
                  "updated": false,
                  "initialized": false
                }
              },
              "commonSetting": {}
            }
          }
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://277fbf090e68:4040/jobs/job?id=418",
              "$$hashKey": "object:24864"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1620671930730_76569006",
      "id": "paragraph_1620671930730_76569006",
      "dateCreated": "2021-05-10T18:38:50+0000",
      "status": "FINISHED",
      "focus": true,
      "$$hashKey": "object:19398",
      "dateFinished": "2021-05-10T18:44:47+0000",
      "dateStarted": "2021-05-10T18:44:46+0000",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TABLE",
            "data": "sum\n22022291\n"
          }
        ]
      }
    },
    {
      "text": "%md\n\n## In case of despair...\n\nIf you get stuck, try the following from a shell in your host OS:\n\n```\ndocker stop hey-spark\ndocker start hey-spark\ndocker exec hey-spark sh -c \"python stream.py &\"\n```\n\nCheck the logs for obvious error notifications or warnings:\n\n```\ndocker logs hey-spark\n```\n\nCheck state of files/directories/etc. inside the container:\n\n```\ndocker exec -it hey-spark /bin/bash\n```\n\nE.g., remove incomplete state from a previous failed attempt:\n\n```\nrm -rf /opt/hadoop/share/runedata /tmp/checkpoint*\n```\n\nStill no luck after checking all of these?\nCome see us for help at the Forum and/or the Matrix room.\n\n_Good luck with the assignment!_",
      "user": "anonymous",
      "dateUpdated": "2021-04-26T19:39:24+0000",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9,
        "editorHide": true,
        "results": {},
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h2>In case of despair&hellip;</h2>\n<p>If you get stuck, try the following from a shell in your host OS:</p>\n<pre><code>docker stop hey-spark\ndocker start hey-spark\ndocker exec hey-spark sh -c &quot;python stream.py &amp;&quot;\n</code></pre>\n<p>Check the logs for obvious error notifications or warnings:</p>\n<pre><code>docker logs hey-spark\n</code></pre>\n<p>Check state of files/directories/etc. inside the container:</p>\n<pre><code>docker exec -it hey-spark /bin/bash\n</code></pre>\n<p>E.g., remove incomplete state from a previous failed attempt:</p>\n<pre><code>rm -rf /opt/hadoop/share/runedata /tmp/checkpoint*\n</code></pre>\n<p>Still no luck after checking all of these?<br />\nCome see us for help at the Forum and/or the Matrix room.</p>\n<p><em>Good luck with the assignment!</em></p>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1619465964525_1344728509",
      "id": "paragraph_1590371520582_267448686",
      "dateCreated": "2021-04-26T19:39:24+0000",
      "status": "READY",
      "$$hashKey": "object:5460"
    }
  ],
  "name": "A5 Sparkling Streams",
  "id": "2G67E9BXS",
  "defaultInterpreterGroup": "spark",
  "version": "0.9.0",
  "noteParams": {},
  "noteForms": {},
  "angularObjects": {},
  "config": {
    "isZeppelinNotebookCronEnable": false,
    "looknfeel": "default",
    "personalizedMode": "false"
  },
  "info": {},
  "path": "/A5 Sparkling Streams"
}