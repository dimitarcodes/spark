{
  "paragraphs": [
    {
      "text": "%md\n# Big Data: Spark RDD\n\n## Getting acquainted with Spark and Spark Notebook\n\n Never used a Notebook? Take a look at the Zeppelin documentation, including a [UI Tour](https://zeppelin.apache.org/docs/0.9.0/quickstart/explore_ui.html), a general Zeppelin notebook [tutorial](https://zeppelin.apache.org/docs/0.9.0/quickstart/tutorial.html) and the [Spark interpreter specific information](https://zeppelin.apache.org/docs/0.9.0/interpreter/spark.html). BTW, our Zeppelin setup is configured on port `:9001` to avoid a conflict with the `:8080` also used by Spark.\n \n Take your time to practice using the Notebook environment, add new cells, split existing ones, switch between code and markdown, _etc. etc._",
      "user": "anonymous",
      "dateUpdated": "2021-04-19T20:34:39+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "colWidth": 12,
        "editorHide": true,
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h1>Big Data: Spark RDD</h1>\n<h2>Getting acquainted with Spark and Spark Notebook</h2>\n<p>Never used a Notebook? Take a look at the Zeppelin documentation, including a <a href=\"https://zeppelin.apache.org/docs/0.9.0/quickstart/explore_ui.html\">UI Tour</a>, a general Zeppelin notebook <a href=\"https://zeppelin.apache.org/docs/0.9.0/quickstart/tutorial.html\">tutorial</a> and the <a href=\"https://zeppelin.apache.org/docs/0.9.0/interpreter/spark.html\">Spark interpreter specific information</a>. BTW, our Zeppelin setup is configured on port <code>:9001</code> to avoid a conflict with the <code>:8080</code> also used by Spark.</p>\n<p>Take your time to practice using the Notebook environment, add new cells, split existing ones, switch between code and markdown, <em>etc. etc.</em></p>\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1618864479746_1618842468",
      "id": "20210323-214004_2105804304",
      "dateCreated": "2021-04-19T20:34:39+0000",
      "status": "READY",
      "focus": true,
      "$$hashKey": "object:278"
    },
    {
      "text": "%md\n_First cell:_\n\nBefore we continue, execute the following cell so Zeppelin initializes the Spark context variables and we can see what versions of Spark and Scala have been installed in the course's docker container that runs the Zeppelin notebook service. Looking up the Spark version is a command against the Spark API, looking up the Scala version is a pure Scala command, as is the printing of both values. _This can take a few seconds but should not take minutes... seek help in the Matrix room if you run into trouble here._",
      "user": "anonymous",
      "dateUpdated": "2021-04-19T20:34:39+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "colWidth": 12,
        "editorHide": true,
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<p><em>First cell:</em></p>\n<p>Before we continue, execute the following cell so Zeppelin initializes the Spark context variables and we can see what versions of Spark and Scala have been installed in the course&rsquo;s docker container that runs the Zeppelin notebook service. Looking up the Spark version is a command against the Spark API, looking up the Scala version is a pure Scala command, as is the printing of both values. <em>This can take a few seconds but should not take minutes&hellip; seek help in the Matrix room if you run into trouble here.</em></p>\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1618864479746_270592694",
      "id": "20210323-214004_2099790480",
      "dateCreated": "2021-04-19T20:34:39+0000",
      "status": "READY",
      "$$hashKey": "object:279"
    },
    {
      "text": "%spark\nprintf(\"Welcome to RU Big Data 2021, the assignment 3 notebook.\\nSpark %s and Scala %s\", sc.version, util.Properties.versionString)",
      "user": "anonymous",
      "dateUpdated": "2021-04-19T20:34:49+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionSupport": true,
          "completionKey": "TAB"
        },
        "editorMode": "ace/mode/scala",
        "colWidth": 12,
        "editorHide": false,
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1618864479746_1888132985",
      "id": "20210323-214004_790036613",
      "dateCreated": "2021-04-19T20:34:39+0000",
      "status": "FINISHED",
      "$$hashKey": "object:280",
      "dateFinished": "2021-04-19T20:34:50+0000",
      "dateStarted": "2021-04-19T20:34:49+0000",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "Welcome to RU Big Data 2021, the assignment 3 notebook.\nSpark 3.1.1 and Scala version 2.12.10"
          }
        ]
      }
    },
    {
      "text": "%md\n## Scala\n\nSpark's API are closely embedded in the hosting language, where you have a choice between Scala, pure Java, and Python. Because (1) Spark has been developed in Scala, (2) it is a nice chance to put our functional programming skills into practice, and (3) Spark is really easier to use with Scala than with the other alternatives (believe me, if you want to really understand what's going on, you want to avoid compiling python to java bytecode on-the-fly), we use the Scala option for the course. In assignment one, you had already the chance to try out some Scala in a Docker container. Now, you can return to the exercises you started then, but using Scala from a Zeppelin notebook instead of using an editor and standalone Scala compiler.\n\n_(Eventually, we will return to compiling Scala before submission a Spark cluster, but only for the final project.)_\n\nPointers to refresh your Scala knowledge, should you need them for inspiration:\n\n+ [Tutorial for Java Programmers](http://docs.scala-lang.org/tutorials/scala-for-java-programmers.html);\n+ Documentation at the main [Scala site](http://docs.scala-lang.org/), including cheatsheets and a tour of scala;\n+ [Library API documentation](https://www.scala-lang.org/api/2.12.13/). \n\n_(Right-click the links to let your browser open those in another Tab. If your browser does not support that, **switch to Firefox** right away, you will not regret.)_",
      "user": "anonymous",
      "dateUpdated": "2021-04-19T20:34:39+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "colWidth": 12,
        "editorHide": true,
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h2>Scala</h2>\n<p>Spark&rsquo;s API are closely embedded in the hosting language, where you have a choice between Scala, pure Java, and Python. Because (1) Spark has been developed in Scala, (2) it is a nice chance to put our functional programming skills into practice, and (3) Spark is really easier to use with Scala than with the other alternatives (believe me, if you want to really understand what&rsquo;s going on, you want to avoid compiling python to java bytecode on-the-fly), we use the Scala option for the course. In assignment one, you had already the chance to try out some Scala in a Docker container. Now, you can return to the exercises you started then, but using Scala from a Zeppelin notebook instead of using an editor and standalone Scala compiler.</p>\n<p><em>(Eventually, we will return to compiling Scala before submission a Spark cluster, but only for the final project.)</em></p>\n<p>Pointers to refresh your Scala knowledge, should you need them for inspiration:</p>\n<ul>\n  <li><a href=\"http://docs.scala-lang.org/tutorials/scala-for-java-programmers.html\">Tutorial for Java Programmers</a>;</li>\n  <li>Documentation at the main <a href=\"http://docs.scala-lang.org/\">Scala site</a>, including cheatsheets and a tour of scala;</li>\n  <li><a href=\"https://www.scala-lang.org/api/2.12.13/\">Library API documentation</a>.</li>\n</ul>\n<p><em>(Right-click the links to let your browser open those in another Tab. If your browser does not support that, <strong>switch to Firefox</strong> right away, you will not regret.)</em></p>\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1618864479749_423682462",
      "id": "20210323-214004_278199093",
      "dateCreated": "2021-04-19T20:34:39+0000",
      "status": "READY",
      "$$hashKey": "object:281"
    },
    {
      "text": "%spark\n// A Scala expression for you to execute:\n1 to 4",
      "user": "anonymous",
      "dateUpdated": "2021-04-19T20:34:53+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionSupport": true,
          "completionKey": "TAB"
        },
        "editorMode": "ace/mode/scala",
        "colWidth": 12,
        "editorHide": false,
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u001b[1m\u001b[34mres60\u001b[0m: \u001b[1m\u001b[32mscala.collection.immutable.Range.Inclusive\u001b[0m = Range 1 to 4\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1618864479749_1710641061",
      "id": "20210323-214004_1540666862",
      "dateCreated": "2021-04-19T20:34:39+0000",
      "status": "FINISHED",
      "$$hashKey": "object:282",
      "dateFinished": "2021-04-19T20:34:53+0000",
      "dateStarted": "2021-04-19T20:34:53+0000"
    },
    {
      "text": "%spark\n// Empty cell for you to try out some more Scala tests here!\n// Add a cell below to create more space for playing around with Scala.\n\nprintln(1 to 4)\n",
      "user": "anonymous",
      "dateUpdated": "2021-04-19T20:35:12+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionSupport": true,
          "completionKey": "TAB"
        },
        "editorMode": "ace/mode/scala",
        "colWidth": 12,
        "editorHide": false,
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "Range 1 to 4\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1618864479749_430087275",
      "id": "20210323-214004_461254562",
      "dateCreated": "2021-04-19T20:34:39+0000",
      "status": "FINISHED",
      "$$hashKey": "object:283",
      "dateFinished": "2021-04-19T20:35:12+0000",
      "dateStarted": "2021-04-19T20:35:12+0000"
    },
    {
      "text": "%md\n_Just in case:_ \nA little Scala background is definitely useful to get things done, but __do not get carried away__, _this_ course is about big data processing, not about functional programming!",
      "user": "anonymous",
      "dateUpdated": "2021-04-19T20:34:39+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "colWidth": 12,
        "editorHide": true,
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<p><em>Just in case:</em><br/>A little Scala background is definitely useful to get things done, but <strong>do not get carried away</strong>, <em>this</em> course is about big data processing, not about functional programming!</p>\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1618864479749_387090388",
      "id": "20210323-214004_1318392341",
      "dateCreated": "2021-04-19T20:34:39+0000",
      "status": "READY",
      "$$hashKey": "object:284"
    },
    {
      "text": "%md\n## Spark\n\nFrom now on, we consider Scala only as a __host language__ for the Spark big data platform. We access Spark from the host language through a special variable, the Spark Context, in these Spark Notebooks available as `sc`. (The Spark interpreter for Zeppelin provides [three more context variables](https://zeppelin.apache.org/docs/0.9.0/interpreter/spark.html#sparkcontext-sqlcontext-sparksession-zeppelincontext), including `spark` for the Data Frame API used in assignment 4.)\n\nThe basic data structure in Spark is the __Resilient Distributed Dataset (RDD)__, that represents collections of items stored _in memory_ on many different computers in the data center (similar to files in Hadoop being represented by one or more blocks in the Hadoop distributed filesystem, RDDs consist of one or more so-called _partitions_ that may reside on different worker nodes).\n\n### Background information\n\nThe following two links give (1) an introduction to using Spark's RDDs to represent collections and (2) the complete programming guide discussing all operations you can apply to RDDs (the latter as a reference to check for more detailed information).\n\n* http://spark.apache.org/examples.html\n* http://spark.apache.org/docs/3.1.1/programming-guide.html\n* https://spark.apache.org/docs/latest/api/scala/org/apache/spark/rdd/RDD.html\n",
      "user": "anonymous",
      "dateUpdated": "2021-04-19T20:34:39+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "colWidth": 12,
        "editorHide": true,
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h2>Spark</h2>\n<p>From now on, we consider Scala only as a <strong>host language</strong> for the Spark big data platform. We access Spark from the host language through a special variable, the Spark Context, in these Spark Notebooks available as <code>sc</code>. (The Spark interpreter for Zeppelin provides <a href=\"https://zeppelin.apache.org/docs/0.9.0/interpreter/spark.html#sparkcontext-sqlcontext-sparksession-zeppelincontext\">three more context variables</a>, including <code>spark</code> for the Data Frame API used in assignment 4.)</p>\n<p>The basic data structure in Spark is the <strong>Resilient Distributed Dataset (RDD)</strong>, that represents collections of items stored <em>in memory</em> on many different computers in the data center (similar to files in Hadoop being represented by one or more blocks in the Hadoop distributed filesystem, RDDs consist of one or more so-called <em>partitions</em> that may reside on different worker nodes).</p>\n<h3>Background information</h3>\n<p>The following two links give (1) an introduction to using Spark&rsquo;s RDDs to represent collections and (2) the complete programming guide discussing all operations you can apply to RDDs (the latter as a reference to check for more detailed information).</p>\n<ul>\n  <li><a href=\"http://spark.apache.org/examples.html\">http://spark.apache.org/examples.html</a></li>\n  <li><a href=\"http://spark.apache.org/docs/3.1.1/programming-guide.html\">http://spark.apache.org/docs/3.1.1/programming-guide.html</a></li>\n  <li><a href=\"https://spark.apache.org/docs/latest/api/scala/org/apache/spark/rdd/RDD.html\">https://spark.apache.org/docs/latest/api/scala/org/apache/spark/rdd/RDD.html</a></li>\n</ul>\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1618864479749_2012717513",
      "id": "20210323-214004_1470434343",
      "dateCreated": "2021-04-19T20:34:39+0000",
      "status": "READY",
      "$$hashKey": "object:285"
    },
    {
      "text": "%md\n### My First RDD\n\nRDDs can be initiated from in-memory collections or from files in the (distributed or local) file system.\n\nLet's first initialize a new RDD from a collection of numbers created by Scala expression `0 to 999` using operation `parallelize` on the `Spark Context`. The second parameter is optional, and instructs the platform to split the data in 8 partitions.\n\nNever hesitate to consult the documentation, e.g. [click here for the `parallelize` docs](http://spark.apache.org/docs/3.1.1/api/scala/org/apache/spark/SparkContext.html#parallelize[T](seq:Seq[T],numSlices:Int)(implicitevidence$1:scala.reflect.ClassTag[T]):org.apache.spark.rdd.RDD[T]). Spark is completely open source, so if the documentation itself is unclear, you can always [look into the code](https://github.com/apache/spark/blob/v3.1.1/core/src/main/scala/org/apache/spark/SparkContext.scala), linked from the top of every API documentation page; [`parallelize` is defined on line 801](https://github.com/apache/spark/blob/1d550c4e90275ab418b9161925049239227f3dc9/core/src/main/scala/org/apache/spark/SparkContext.scala#L801).\n\n_Now try that for resolving your Windows bugs!_\n",
      "user": "anonymous",
      "dateUpdated": "2021-04-19T20:34:39+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "colWidth": 12,
        "editorHide": true,
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h3>My First RDD</h3>\n<p>RDDs can be initiated from in-memory collections or from files in the (distributed or local) file system.</p>\n<p>Let&rsquo;s first initialize a new RDD from a collection of numbers created by Scala expression <code>0 to 999</code> using operation <code>parallelize</code> on the <code>Spark Context</code>. The second parameter is optional, and instructs the platform to split the data in 8 partitions.</p>\n<p>Never hesitate to consult the documentation, e.g. [click here for the <code>parallelize</code> docs](<a href=\"http://spark.apache.org/docs/3.1.1/api/scala/org/apache/spark/SparkContext.html#parallelize[T](seq:Seq[T],numSlices:Int)(implicitevidence$1:scala.reflect.ClassTag[T]):org.apache.spark.rdd.RDD[T])\">http://spark.apache.org/docs/3.1.1/api/scala/org/apache/spark/SparkContext.html#parallelize[T](seq:Seq[T],numSlices:Int)(implicitevidence$1:scala.reflect.ClassTag[T]):org.apache.spark.rdd.RDD[T])</a>. Spark is completely open source, so if the documentation itself is unclear, you can always <a href=\"https://github.com/apache/spark/blob/v3.1.1/core/src/main/scala/org/apache/spark/SparkContext.scala\">look into the code</a>, linked from the top of every API documentation page; <a href=\"https://github.com/apache/spark/blob/1d550c4e90275ab418b9161925049239227f3dc9/core/src/main/scala/org/apache/spark/SparkContext.scala#L801\"><code>parallelize</code> is defined on line 801</a>.</p>\n<p><em>Now try that for resolving your Windows bugs!</em></p>\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1618864479749_1454068667",
      "id": "20210323-214004_314540352",
      "dateCreated": "2021-04-19T20:34:39+0000",
      "status": "READY",
      "$$hashKey": "object:286"
    },
    {
      "text": "%spark\nval rdd = sc.parallelize(0 to 999,8)",
      "user": "anonymous",
      "dateUpdated": "2021-04-19T20:35:21+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionSupport": true,
          "completionKey": "TAB"
        },
        "editorMode": "ace/mode/scala",
        "colWidth": 12,
        "editorHide": false,
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u001b[1m\u001b[34mrdd\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.rdd.RDD[Int]\u001b[0m = ParallelCollectionRDD[42] at parallelize at <console>:29\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1618864479749_1505759359",
      "id": "20210323-214004_1953166140",
      "dateCreated": "2021-04-19T20:34:39+0000",
      "status": "FINISHED",
      "$$hashKey": "object:287",
      "dateFinished": "2021-04-19T20:35:21+0000",
      "dateStarted": "2021-04-19T20:35:21+0000"
    },
    {
      "text": "%md\n### Lazy evaluation\n\nEvaluation of operations in Spark is lazy - only operations that require output to be materialized will actually trigger execution. Remember that evaluation is lazy, and only happens upon actions, not transformations; i.e., so far, nothing happened.\n\nIf you check the Spark UI, you find that the [stages tab](http://localhost:4040/stages/) is still empty _(right-click open tab...)_.",
      "user": "anonymous",
      "dateUpdated": "2021-04-19T20:34:39+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "colWidth": 12,
        "editorHide": true,
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h3>Lazy evaluation</h3>\n<p>Evaluation of operations in Spark is lazy - only operations that require output to be materialized will actually trigger execution. Remember that evaluation is lazy, and only happens upon actions, not transformations; i.e., so far, nothing happened.</p>\n<p>If you check the Spark UI, you find that the <a href=\"http://localhost:4040/stages/\">stages tab</a> is still empty <em>(right-click open tab&hellip;)</em>.</p>\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1618864479749_2129672933",
      "id": "20210323-214004_655089005",
      "dateCreated": "2021-04-19T20:34:39+0000",
      "status": "READY",
      "$$hashKey": "object:288"
    },
    {
      "text": "%spark\nval sample = rdd.takeSample(false, 4)",
      "user": "anonymous",
      "dateUpdated": "2021-04-19T20:35:24+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionSupport": true,
          "completionKey": "TAB"
        },
        "editorMode": "ace/mode/scala",
        "colWidth": 12,
        "editorHide": false,
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u001b[1m\u001b[34msample\u001b[0m: \u001b[1m\u001b[32mArray[Int]\u001b[0m = Array(835, 914, 709, 966)\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://277fbf090e68:4040/jobs/job?id=17",
              "$$hashKey": "object:4612"
            },
            {
              "jobUrl": "http://277fbf090e68:4040/jobs/job?id=18",
              "$$hashKey": "object:4613"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1618864479749_1007178674",
      "id": "20210323-214004_1733936473",
      "dateCreated": "2021-04-19T20:34:39+0000",
      "status": "FINISHED",
      "$$hashKey": "object:289",
      "dateFinished": "2021-04-19T20:35:24+0000",
      "dateStarted": "2021-04-19T20:35:24+0000"
    },
    {
      "text": "%md\nOnly now, evaluation took place: see the [stages](http://localhost:4040/stages/) in the Spark UI.\nClick on the links, also check out the [jobs](http://localhost:4040/jobs/)!\n\nModerate question: _Why did Spark fire off eight different tasks?_ \n\nModerate answer: _Because the RDD was initialized with 2nd argument 8 in the parallelize function, meaning there are 8 workers working on 8 partitions_\n\nEvery student in the course should be able to figure out the answer to this question. I also have an advanced question to think about, for those students who want to dig deep: _Why would Spark create two jobs to take the sample?_ The answer is not that easy to find actually, you will have to [read the `takeSample` code](https://github.com/apache/spark/blob/1d550c4e90275ab418b9161925049239227f3dc9/core/src/main/scala/org/apache/spark/rdd/RDD.scala#L620).\n\nAnswer: Sometimes spark may not grab the correct number of samples the first time around, resulting in a 2nd job? i think\n_It's totally okay to skip the advanced question and continue with the next cell._",
      "user": "anonymous",
      "dateUpdated": "2021-04-19T20:39:16+0000",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionSupport": false
        },
        "editorMode": "ace/mode/markdown",
        "colWidth": 12,
        "editorHide": true,
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<p>Only now, evaluation took place: see the <a href=\"http://localhost:4040/stages/\">stages</a> in the Spark UI.<br />\nClick on the links, also check out the <a href=\"http://localhost:4040/jobs/\">jobs</a>!</p>\n<p>Moderate question: <em>Why did Spark fire off eight different tasks?</em></p>\n<p>Moderate answer: <em>Because the RDD was initialized with 2nd argument 8 in the parallelize function, meaning there are 8 workers working on 8 partitions</em></p>\n<p>Every student in the course should be able to figure out the answer to this question. I also have an advanced question to think about, for those students who want to dig deep: <em>Why would Spark create two jobs to take the sample?</em> The answer is not that easy to find actually, you will have to <a href=\"https://github.com/apache/spark/blob/1d550c4e90275ab418b9161925049239227f3dc9/core/src/main/scala/org/apache/spark/rdd/RDD.scala#L620\">read the <code>takeSample</code> code</a>.</p>\n<p>Answer: Sometimes spark may not grab the correct number of samples the first time around, resulting in a 2nd job? i think<br />\n<em>It&rsquo;s totally okay to skip the advanced question and continue with the next cell.</em></p>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1618864479750_662611403",
      "id": "20210323-214004_1350851909",
      "dateCreated": "2021-04-19T20:34:39+0000",
      "status": "FINISHED",
      "$$hashKey": "object:290",
      "dateFinished": "2021-04-19T20:39:16+0000",
      "dateStarted": "2021-04-19T20:39:16+0000"
    },
    {
      "text": "%md\n### Loading data\n\nYou can use a shell escape to download the Gutenberg data into the container where you run this Notebook. You only have to do this once, but let's guard with a simple test to not load the data twice if it's already there. _Of course, it's perfectly fine to copy the data from your host onto the container using `docker cp` and save some bandwidth._",
      "user": "anonymous",
      "dateUpdated": "2021-04-19T20:34:39+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "colWidth": 12,
        "editorHide": true,
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h3>Loading data</h3>\n<p>You can use a shell escape to download the Gutenberg data into the container where you run this Notebook. You only have to do this once, but let&rsquo;s guard with a simple test to not load the data twice if it&rsquo;s already there. <em>Of course, it&rsquo;s perfectly fine to copy the data from your host onto the container using <code>docker cp</code> and save some bandwidth.</em></p>\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1618864479750_1508559654",
      "id": "20210323-214004_866898295",
      "dateCreated": "2021-04-19T20:34:39+0000",
      "status": "READY",
      "$$hashKey": "object:291"
    },
    {
      "text": "%sh\necho Download Gutenberg data...\ncd /opt/hadoop\n[ ! -f 100.txt ] && wget --quiet https://raw.githubusercontent.com/rubigdata-dockerhub/hadoop-dockerfile/master/100.txt && echo Downloaded || echo File already exists",
      "user": "anonymous",
      "dateUpdated": "2021-04-19T20:39:20+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "sh",
          "editOnDblClick": false,
          "completionSupport": false
        },
        "editorMode": "ace/mode/sh",
        "colWidth": 12,
        "editorHide": false,
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "Download Gutenberg data...\nFile already exists\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1618864479750_222836697",
      "id": "20210323-214005_1492206213",
      "dateCreated": "2021-04-19T20:34:39+0000",
      "status": "FINISHED",
      "$$hashKey": "object:292",
      "dateFinished": "2021-04-19T20:39:20+0000",
      "dateStarted": "2021-04-19T20:39:20+0000"
    },
    {
      "text": "%md\n### Counting words\n\nWe return to the Shakespeare data that we processed already using Map-Reduce for the classic Big Data \"Hello World\" exercise, counting words.\n",
      "user": "anonymous",
      "dateUpdated": "2021-04-19T20:34:39+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "colWidth": 12,
        "editorHide": true,
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h3>Counting words</h3>\n<p>We return to the Shakespeare data that we processed already using Map-Reduce for the classic Big Data &ldquo;Hello World&rdquo; exercise, counting words.</p>\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1618864479750_221429330",
      "id": "20210323-214005_1346850954",
      "dateCreated": "2021-04-19T20:34:39+0000",
      "status": "READY",
      "$$hashKey": "object:293"
    },
    {
      "text": "%spark\nval lines = sc.textFile(\"file:///opt/hadoop/100.txt\")",
      "user": "anonymous",
      "dateUpdated": "2021-04-19T20:39:23+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionSupport": true,
          "completionKey": "TAB"
        },
        "editorMode": "ace/mode/scala",
        "colWidth": 12,
        "editorHide": false,
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u001b[1m\u001b[34mlines\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.rdd.RDD[String]\u001b[0m = file:///opt/hadoop/100.txt MapPartitionsRDD[45] at textFile at <console>:29\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1618864479750_714230766",
      "id": "20210323-214005_2118819986",
      "dateCreated": "2021-04-19T20:34:39+0000",
      "status": "FINISHED",
      "$$hashKey": "object:294",
      "dateFinished": "2021-04-19T20:39:24+0000",
      "dateStarted": "2021-04-19T20:39:23+0000"
    },
    {
      "text": "%spark\nlines.count",
      "user": "anonymous",
      "dateUpdated": "2021-04-19T20:39:26+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionSupport": true,
          "completionKey": "TAB"
        },
        "editorMode": "ace/mode/scala",
        "colWidth": 12,
        "editorHide": false,
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u001b[1m\u001b[34mres62\u001b[0m: \u001b[1m\u001b[32mLong\u001b[0m = 147838\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://277fbf090e68:4040/jobs/job?id=19",
              "$$hashKey": "object:4765"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1618864479750_151547145",
      "id": "20210323-214005_558154632",
      "dateCreated": "2021-04-19T20:34:39+0000",
      "status": "FINISHED",
      "$$hashKey": "object:295",
      "dateFinished": "2021-04-19T20:39:28+0000",
      "dateStarted": "2021-04-19T20:39:26+0000"
    },
    {
      "text": "%md\nCan you predict what the following commands will do?\nRecognize the Map Reduce pattern on lines 2 and 3?\n\nAnswer: Yes i can predict - it will first print the number lines by counting the number of elements in the RDD lines. Then it will print the total number of characters by first mapping every element in lines (every line s) to its number of characters (s.length) and then summing eahc pair of lengths.\n\nRemember that Scala is a functional language; the mapper and reducer get _functions as parameters_. The mapper is given a lambda function that transforms a sentence `s` into its length in characters (`s.length`), where the reducer receives a function that is applied to _fold_ the list of values (one such list for every key). \n\n_Note:_ if you never took a functional programming course, look at [this answer on StackExchange](http://stackoverflow.com/a/16509/2127435) to understand the concept of a _lambda function_.",
      "user": "anonymous",
      "dateUpdated": "2021-04-19T20:41:17+0000",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionSupport": false
        },
        "editorMode": "ace/mode/markdown",
        "colWidth": 12,
        "editorHide": true,
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<p>Can you predict what the following commands will do?<br />\nRecognize the Map Reduce pattern on lines 2 and 3?</p>\n<p>Answer: Yes i can predict - it will first print the number lines by counting the number of elements in the RDD lines. Then it will print the total number of characters by first mapping every element in lines (every line s) to its number of characters (s.length) and then summing eahc pair of lengths.</p>\n<p>Remember that Scala is a functional language; the mapper and reducer get <em>functions as parameters</em>. The mapper is given a lambda function that transforms a sentence <code>s</code> into its length in characters (<code>s.length</code>), where the reducer receives a function that is applied to <em>fold</em> the list of values (one such list for every key).</p>\n<p><em>Note:</em> if you never took a functional programming course, look at <a href=\"http://stackoverflow.com/a/16509/2127435\">this answer on StackExchange</a> to understand the concept of a <em>lambda function</em>.</p>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1618864479750_716555179",
      "id": "20210323-214005_1936934991",
      "dateCreated": "2021-04-19T20:34:39+0000",
      "status": "FINISHED",
      "$$hashKey": "object:296",
      "dateFinished": "2021-04-19T20:41:17+0000",
      "dateStarted": "2021-04-19T20:41:17+0000"
    },
    {
      "text": "%spark\nprintln( \"Lines:\\t\", lines.count, \"\\n\" + \n         \"Chars:\\t\", lines.map(s => s.length).\n                           reduce((v_i, v_j) => v_i + v_j))",
      "user": "anonymous",
      "dateUpdated": "2021-04-19T20:34:39+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionSupport": true,
          "completionKey": "TAB"
        },
        "editorMode": "ace/mode/scala",
        "colWidth": 12,
        "editorHide": false,
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1618864479750_1730205913",
      "id": "20210323-214005_1402180914",
      "dateCreated": "2021-04-19T20:34:39+0000",
      "status": "READY",
      "$$hashKey": "object:297"
    },
    {
      "text": "%md\nSo, the map operator executes its parameter, the lambda function, on every item in the RDD. Reduce is also defined using a lambda function that consumes pairs of values.\n\n_Exercise:_ find the length of the longest sentence in the corpus, knowing that in scala you can write `4 max 6` to get 6 as their maximum.",
      "user": "anonymous",
      "dateUpdated": "2021-04-19T20:34:39+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "colWidth": 12,
        "editorHide": true,
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<p>So, the map operator executes its parameter, the lambda function, on every item in the RDD. Reduce is also defined using a lambda function that consumes pairs of values.</p>\n<p><em>Exercise:</em> find the length of the longest sentence in the corpus, knowing that in scala you can write <code>4 max 6</code> to get 6 as their maximum.</p>\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1618864479750_305900529",
      "id": "20210323-214005_1265751118",
      "dateCreated": "2021-04-19T20:34:39+0000",
      "status": "READY",
      "$$hashKey": "object:298"
    },
    {
      "text": "%spark\n// Empty cell for the exercise\nprintln(\"Longest sentence : \\t\", lines.map(s=>s.length).reduce((v_i, v_j) => v_i max v_j))",
      "user": "anonymous",
      "dateUpdated": "2021-04-19T20:41:46+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionSupport": true,
          "completionKey": "TAB"
        },
        "editorMode": "ace/mode/scala",
        "colWidth": 12,
        "editorHide": false,
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "(Longest sentence : \t,78)\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://277fbf090e68:4040/jobs/job?id=20",
              "$$hashKey": "object:4825"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1618864479750_696469462",
      "id": "20210323-214005_1294421539",
      "dateCreated": "2021-04-19T20:34:39+0000",
      "status": "FINISHED",
      "$$hashKey": "object:299",
      "dateFinished": "2021-04-19T20:41:46+0000",
      "dateStarted": "2021-04-19T20:41:46+0000"
    },
    {
      "text": "%md\nNow try to understand in detail the following example.\n_Try to understand why we used `flatMap` and not `map`._\n\nWe used flatmap because we want to produce multiple output items (words) out of every input item (lines).\nMap can only be used to map items one to one. flatMap can be used to map items one to many, one to one or one to none. \n\nIt is worth copying the cell, and inspecting output at intermediate steps (use `take()`, not `collect()`.",
      "user": "anonymous",
      "dateUpdated": "2021-04-19T20:42:57+0000",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionSupport": false
        },
        "editorMode": "ace/mode/markdown",
        "colWidth": 12,
        "editorHide": true,
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<p>Now try to understand in detail the following example.<br />\n<em>Try to understand why we used <code>flatMap</code> and not <code>map</code>.</em></p>\n<p>We used flatmap because we want to produce multiple output items (words) out of every input item (lines).<br />\nMap can only be used to map items one to one. flatMap can be used to map items one to many, one to one or one to none.</p>\n<p>It is worth copying the cell, and inspecting output at intermediate steps (use <code>take()</code>, not <code>collect()</code>.</p>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1618864479750_1151533674",
      "id": "20210323-214005_1924995235",
      "dateCreated": "2021-04-19T20:34:39+0000",
      "status": "FINISHED",
      "$$hashKey": "object:300",
      "dateFinished": "2021-04-19T20:42:57+0000",
      "dateStarted": "2021-04-19T20:42:57+0000"
    },
    {
      "text": "%spark\nval words = lines.flatMap(line => line.split(\" \"))\n              .filter(_ != \"\")\n              .map(word => (word,1))",
      "user": "anonymous",
      "dateUpdated": "2021-04-19T20:43:00+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionSupport": true,
          "completionKey": "TAB"
        },
        "editorMode": "ace/mode/scala",
        "colWidth": 12,
        "editorHide": false,
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u001b[1m\u001b[34mwords\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.rdd.RDD[(String, Int)]\u001b[0m = MapPartitionsRDD[49] at map at <console>:31\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1618864479750_361473421",
      "id": "20210323-214005_631504163",
      "dateCreated": "2021-04-19T20:34:39+0000",
      "status": "FINISHED",
      "$$hashKey": "object:301",
      "dateFinished": "2021-04-19T20:43:00+0000",
      "dateStarted": "2021-04-19T20:43:00+0000"
    },
    {
      "text": "%spark\nval wc = words.reduceByKey(_ + _)",
      "user": "anonymous",
      "dateUpdated": "2021-04-19T20:43:02+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionSupport": true,
          "completionKey": "TAB"
        },
        "editorMode": "ace/mode/scala",
        "colWidth": 12,
        "editorHide": false,
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u001b[1m\u001b[34mwc\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.rdd.RDD[(String, Int)]\u001b[0m = ShuffledRDD[50] at reduceByKey at <console>:29\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1618864479751_773462759",
      "id": "20210323-214005_2022723396",
      "dateCreated": "2021-04-19T20:34:39+0000",
      "status": "FINISHED",
      "$$hashKey": "object:302",
      "dateFinished": "2021-04-19T20:43:03+0000",
      "dateStarted": "2021-04-19T20:43:02+0000"
    },
    {
      "text": "%spark\nwc.take(10)",
      "user": "anonymous",
      "dateUpdated": "2021-04-19T20:43:05+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionSupport": true,
          "completionKey": "TAB"
        },
        "editorMode": "ace/mode/scala",
        "colWidth": 12,
        "editorHide": false,
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u001b[1m\u001b[34mres64\u001b[0m: \u001b[1m\u001b[32mArray[(String, Int)]\u001b[0m = Array((hack'd.,1), (durst,,2), (Ah!,6), (Worthy;,1), (bone,6), (vailing,1), (bombast,1), (person-,1), (LAFEU],1), (fiction.,1))\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://277fbf090e68:4040/jobs/job?id=21",
              "$$hashKey": "object:4935"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1618864479751_2117909912",
      "id": "20210323-214005_746500975",
      "dateCreated": "2021-04-19T20:34:39+0000",
      "status": "FINISHED",
      "$$hashKey": "object:303",
      "dateFinished": "2021-04-19T20:43:07+0000",
      "dateStarted": "2021-04-19T20:43:05+0000"
    },
    {
      "text": "%md\nYou can inspect how the platform processes this query by inspecting the _query plan_ using `toDebugString`.",
      "user": "anonymous",
      "dateUpdated": "2021-04-19T20:34:39+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "colWidth": 12,
        "editorHide": true,
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<p>You can inspect how the platform processes this query by inspecting the <em>query plan</em> using <code>toDebugString</code>.</p>\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1618864479751_865630000",
      "id": "20210323-214005_1198946825",
      "dateCreated": "2021-04-19T20:34:39+0000",
      "status": "READY",
      "$$hashKey": "object:304"
    },
    {
      "text": "%spark\nwc.toDebugString",
      "user": "anonymous",
      "dateUpdated": "2021-04-19T20:43:10+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionSupport": true,
          "completionKey": "TAB"
        },
        "editorMode": "ace/mode/scala",
        "colWidth": 12,
        "editorHide": false,
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u001b[1m\u001b[34mres65\u001b[0m: \u001b[1m\u001b[32mString\u001b[0m =\n(2) ShuffledRDD[50] at reduceByKey at <console>:29 []\n +-(2) MapPartitionsRDD[49] at map at <console>:31 []\n    |  MapPartitionsRDD[48] at filter at <console>:30 []\n    |  MapPartitionsRDD[47] at flatMap at <console>:29 []\n    |  file:///opt/hadoop/100.txt MapPartitionsRDD[45] at textFile at <console>:29 []\n    |  file:///opt/hadoop/100.txt HadoopRDD[44] at textFile at <console>:29 []\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1618864479751_1036372716",
      "id": "20210323-214005_2105986337",
      "dateCreated": "2021-04-19T20:34:39+0000",
      "status": "FINISHED",
      "$$hashKey": "object:305",
      "dateFinished": "2021-04-19T20:43:10+0000",
      "dateStarted": "2021-04-19T20:43:10+0000"
    },
    {
      "text": "%md\nAlternatively, you can look at the represenation of the DAG and decomposition of the job into tasks using the Spark UI → [see stages](http://localhost:4040/stages/) and their constituent tasks.",
      "user": "anonymous",
      "dateUpdated": "2021-04-19T20:34:39+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "colWidth": 12,
        "editorHide": true,
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<p>Alternatively, you can look at the represenation of the DAG and decomposition of the job into tasks using the Spark UI → <a href=\"http://localhost:4040/stages/\">see stages</a> and their constituent tasks.</p>\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1618864479751_138951393",
      "id": "20210323-214005_127289274",
      "dateCreated": "2021-04-19T20:34:39+0000",
      "status": "READY",
      "$$hashKey": "object:306"
    },
    {
      "text": "%md\n### To count or not to count\nOk, we can count words - let us find out which words Shakespeare used most often!",
      "user": "anonymous",
      "dateUpdated": "2021-04-19T20:34:39+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "colWidth": 12,
        "editorHide": true,
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h3>To count or not to count</h3>\n<p>Ok, we can count words - let us find out which words Shakespeare used most often!</p>\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1618864479751_605835429",
      "id": "20210323-214005_882804512",
      "dateCreated": "2021-04-19T20:34:39+0000",
      "status": "READY",
      "$$hashKey": "object:307"
    },
    {
      "text": "%spark\nval top10 = wc.takeOrdered(10)",
      "user": "anonymous",
      "dateUpdated": "2021-04-19T20:43:14+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionSupport": true,
          "completionKey": "TAB"
        },
        "editorMode": "ace/mode/scala",
        "colWidth": 12,
        "editorHide": false,
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u001b[1m\u001b[34mtop10\u001b[0m: \u001b[1m\u001b[32mArray[(String, Int)]\u001b[0m = Array((\"\tTom’s\",1), (\",241), (\"'Tis,1), (\"A,3), (\"Air,\",1), (\"Alas,,1), (\"Amen\",2), (\"Amen\"?,1), (\"Amen,\",1), (\"And,1))\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://277fbf090e68:4040/jobs/job?id=22",
              "$$hashKey": "object:4996"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1618864479751_897751149",
      "id": "20210323-214005_1243121125",
      "dateCreated": "2021-04-19T20:34:39+0000",
      "status": "FINISHED",
      "$$hashKey": "object:308",
      "dateFinished": "2021-04-19T20:43:14+0000",
      "dateStarted": "2021-04-19T20:43:14+0000"
    },
    {
      "text": "%md\nOk, not quite what we wanted!\nSee what's wrong?\n\n\nIf you don't... on my desktop, I'd open a shell and run `man ascii` but you can also look at an online [ASCII table](https://www.cs.cmu.edu/~pattis/15-1XX/common/handouts/ascii.html).\n\nLet's fix the result ordering as follows.",
      "user": "anonymous",
      "dateUpdated": "2021-04-19T20:43:49+0000",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionSupport": false
        },
        "editorMode": "ace/mode/markdown",
        "colWidth": 12,
        "editorHide": true,
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<p>Ok, not quite what we wanted!<br />\nSee what&rsquo;s wrong?</p>\n<p>If you don&rsquo;t&hellip; on my desktop, I&rsquo;d open a shell and run <code>man ascii</code> but you can also look at an online <a href=\"https://www.cs.cmu.edu/~pattis/15-1XX/common/handouts/ascii.html\">ASCII table</a>.</p>\n<p>Let&rsquo;s fix the result ordering as follows.</p>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1618864479751_1183319920",
      "id": "20210323-214005_397633229",
      "dateCreated": "2021-04-19T20:34:39+0000",
      "status": "FINISHED",
      "$$hashKey": "object:309",
      "dateFinished": "2021-04-19T20:43:49+0000",
      "dateStarted": "2021-04-19T20:43:49+0000"
    },
    {
      "text": "%spark\nval top10 = wc.takeOrdered(10)(Ordering[Int].reverse.on(x=>x._2))",
      "user": "anonymous",
      "dateUpdated": "2021-04-19T20:43:51+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionSupport": true,
          "completionKey": "TAB"
        },
        "editorMode": "ace/mode/scala",
        "colWidth": 12,
        "editorHide": false,
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u001b[1m\u001b[34mtop10\u001b[0m: \u001b[1m\u001b[32mArray[(String, Int)]\u001b[0m = Array((the,25378), (I,20629), (and,19806), (to,16966), (of,16718), (a,13657), (my,11443), (in,10519), (you,9591), (is,8335))\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://277fbf090e68:4040/jobs/job?id=23",
              "$$hashKey": "object:5056"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1618864479751_1445117669",
      "id": "20210323-214005_764153978",
      "dateCreated": "2021-04-19T20:34:39+0000",
      "status": "FINISHED",
      "$$hashKey": "object:310",
      "dateFinished": "2021-04-19T20:43:52+0000",
      "dateStarted": "2021-04-19T20:43:51+0000"
    },
    {
      "text": "%md\nYou can render the collected results in any way you want to, just use the power of the client programming language (i.e., Scala, after the `collect`).",
      "user": "anonymous",
      "dateUpdated": "2021-04-19T20:34:39+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "colWidth": 12,
        "editorHide": true,
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<p>You can render the collected results in any way you want to, just use the power of the client programming language (i.e., Scala, after the <code>collect</code>).</p>\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1618864479751_1947245493",
      "id": "20210323-214005_1835738476",
      "dateCreated": "2021-04-19T20:34:39+0000",
      "status": "READY",
      "$$hashKey": "object:311"
    },
    {
      "text": "%spark\ntop10.map({case(w,c) => \"Word '%s' occurs %d times\".format(w,c)}).map(println)",
      "user": "anonymous",
      "dateUpdated": "2021-04-19T20:43:54+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionSupport": true,
          "completionKey": "TAB"
        },
        "editorMode": "ace/mode/scala",
        "colWidth": 12,
        "editorHide": false,
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "Word 'the' occurs 25378 times\nWord 'I' occurs 20629 times\nWord 'and' occurs 19806 times\nWord 'to' occurs 16966 times\nWord 'of' occurs 16718 times\nWord 'a' occurs 13657 times\nWord 'my' occurs 11443 times\nWord 'in' occurs 10519 times\nWord 'you' occurs 9591 times\nWord 'is' occurs 8335 times\n\u001b[1m\u001b[34mres66\u001b[0m: \u001b[1m\u001b[32mArray[Unit]\u001b[0m = Array((), (), (), (), (), (), (), (), (), ())\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1618864479751_2079105232",
      "id": "20210323-214005_1674748553",
      "dateCreated": "2021-04-19T20:34:39+0000",
      "status": "FINISHED",
      "$$hashKey": "object:312",
      "dateFinished": "2021-04-19T20:43:54+0000",
      "dateStarted": "2021-04-19T20:43:54+0000"
    },
    {
      "text": "%md\nWe can zoom in on specific word frequencies, that might be more interesting than stopwords!",
      "user": "anonymous",
      "dateUpdated": "2021-04-19T20:34:39+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "colWidth": 12,
        "editorHide": true,
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<p>We can zoom in on specific word frequencies, that might be more interesting than stopwords!</p>\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1618864479751_515518972",
      "id": "20210323-214005_1876632880",
      "dateCreated": "2021-04-19T20:34:39+0000",
      "status": "READY",
      "$$hashKey": "object:313"
    },
    {
      "text": "%spark\nwc.filter(_._1 == \"Romeo\").collect",
      "user": "anonymous",
      "dateUpdated": "2021-04-19T20:43:58+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionSupport": true,
          "completionKey": "TAB"
        },
        "editorMode": "ace/mode/scala",
        "colWidth": 12,
        "editorHide": false,
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u001b[1m\u001b[34mres67\u001b[0m: \u001b[1m\u001b[32mArray[(String, Int)]\u001b[0m = Array((Romeo,45))\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://277fbf090e68:4040/jobs/job?id=24",
              "$$hashKey": "object:5117"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1618864479752_1165726102",
      "id": "20210323-214005_572127089",
      "dateCreated": "2021-04-19T20:34:39+0000",
      "status": "FINISHED",
      "$$hashKey": "object:314",
      "dateFinished": "2021-04-19T20:43:59+0000",
      "dateStarted": "2021-04-19T20:43:58+0000"
    },
    {
      "text": "%spark\nwc.filter(_._1 == \"Julia\").collect",
      "user": "anonymous",
      "dateUpdated": "2021-04-19T20:44:02+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionSupport": true,
          "completionKey": "TAB"
        },
        "editorMode": "ace/mode/scala",
        "colWidth": 12,
        "editorHide": false,
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u001b[1m\u001b[34mres68\u001b[0m: \u001b[1m\u001b[32mArray[(String, Int)]\u001b[0m = Array((Julia,12))\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://277fbf090e68:4040/jobs/job?id=25",
              "$$hashKey": "object:5153"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1618864479752_1593184287",
      "id": "20210323-214005_384693787",
      "dateCreated": "2021-04-19T20:34:39+0000",
      "status": "FINISHED",
      "$$hashKey": "object:315",
      "dateFinished": "2021-04-19T20:44:02+0000",
      "dateStarted": "2021-04-19T20:44:02+0000"
    },
    {
      "text": "%spark\nwc.cache()",
      "user": "anonymous",
      "dateUpdated": "2021-04-19T20:44:04+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionSupport": true,
          "completionKey": "TAB"
        },
        "editorMode": "ace/mode/scala",
        "colWidth": 12,
        "editorHide": false,
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u001b[1m\u001b[34mres69\u001b[0m: \u001b[1m\u001b[32mwc.type\u001b[0m = ShuffledRDD[50] at reduceByKey at <console>:29\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1618864479752_283664859",
      "id": "20210323-214005_1625147557",
      "dateCreated": "2021-04-19T20:34:39+0000",
      "status": "FINISHED",
      "$$hashKey": "object:316",
      "dateFinished": "2021-04-19T20:44:04+0000",
      "dateStarted": "2021-04-19T20:44:04+0000"
    },
    {
      "text": "%spark\nwc.filter(_._1 == \"Macbeth\").collect",
      "user": "anonymous",
      "dateUpdated": "2021-04-19T20:44:07+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionSupport": true,
          "completionKey": "TAB"
        },
        "editorMode": "ace/mode/scala",
        "colWidth": 12,
        "editorHide": false,
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u001b[1m\u001b[34mres70\u001b[0m: \u001b[1m\u001b[32mArray[(String, Int)]\u001b[0m = Array((Macbeth,30))\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://277fbf090e68:4040/jobs/job?id=26",
              "$$hashKey": "object:5214"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1618864479752_1394164839",
      "id": "20210323-214005_1444516651",
      "dateCreated": "2021-04-19T20:34:39+0000",
      "status": "FINISHED",
      "$$hashKey": "object:317",
      "dateFinished": "2021-04-19T20:44:07+0000",
      "dateStarted": "2021-04-19T20:44:07+0000"
    },
    {
      "text": "%spark\nwc.filter(_._1 == \"Capulet\").collect",
      "user": "anonymous",
      "dateUpdated": "2021-04-19T20:44:09+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionSupport": true,
          "completionKey": "TAB"
        },
        "editorMode": "ace/mode/scala",
        "colWidth": 12,
        "editorHide": false,
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u001b[1m\u001b[34mres71\u001b[0m: \u001b[1m\u001b[32mArray[(String, Int)]\u001b[0m = Array((Capulet,9))\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://277fbf090e68:4040/jobs/job?id=27",
              "$$hashKey": "object:5250"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1618864479752_1646119429",
      "id": "20210323-214005_995509478",
      "dateCreated": "2021-04-19T20:34:39+0000",
      "status": "FINISHED",
      "$$hashKey": "object:318",
      "dateFinished": "2021-04-19T20:44:09+0000",
      "dateStarted": "2021-04-19T20:44:09+0000"
    },
    {
      "text": "%md\nMany different ways exist to compute the top N results. A few follow - _try to understand what actual work (for the cluster) is actually generated by the various alternatives._",
      "user": "anonymous",
      "dateUpdated": "2021-04-19T20:34:39+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "colWidth": 12,
        "editorHide": true,
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<p>Many different ways exist to compute the top N results. A few follow - <em>try to understand what actual work (for the cluster) is actually generated by the various alternatives.</em></p>\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1618864479752_2138706112",
      "id": "20210323-214005_1123164345",
      "dateCreated": "2021-04-19T20:34:39+0000",
      "status": "READY",
      "$$hashKey": "object:319"
    },
    {
      "text": "%spark\nval oCounts = wc.map(x => x._2 -> x._1).sortByKey(false).map(x => x._2 -> x._1).cache()\noCounts.take(10).foreach(println)",
      "user": "anonymous",
      "dateUpdated": "2021-04-19T20:44:12+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionSupport": true,
          "completionKey": "TAB"
        },
        "editorMode": "ace/mode/scala",
        "colWidth": 12,
        "editorHide": false,
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "(the,25378)\n(I,20629)\n(and,19806)\n(to,16966)\n(of,16718)\n(a,13657)\n(my,11443)\n(in,10519)\n(you,9591)\n(is,8335)\n\u001b[1m\u001b[34moCounts\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.rdd.RDD[(String, Int)]\u001b[0m = MapPartitionsRDD[61] at map at <console>:29\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://277fbf090e68:4040/jobs/job?id=28",
              "$$hashKey": "object:5298"
            },
            {
              "jobUrl": "http://277fbf090e68:4040/jobs/job?id=29",
              "$$hashKey": "object:5299"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1618864479752_2089305110",
      "id": "20210323-214005_1290031266",
      "dateCreated": "2021-04-19T20:34:39+0000",
      "status": "FINISHED",
      "$$hashKey": "object:320",
      "dateFinished": "2021-04-19T20:44:13+0000",
      "dateStarted": "2021-04-19T20:44:12+0000"
    },
    {
      "text": "%spark\n// Alternative way to achieve the same:\nwc.sortBy(x => -x._2).take(10).foreach(println)",
      "user": "anonymous",
      "dateUpdated": "2021-04-19T20:44:15+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionSupport": true,
          "completionKey": "TAB"
        },
        "editorMode": "ace/mode/scala",
        "colWidth": 12,
        "editorHide": false,
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "(the,25378)\n(I,20629)\n(and,19806)\n(to,16966)\n(of,16718)\n(a,13657)\n(my,11443)\n(in,10519)\n(you,9591)\n(is,8335)\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://277fbf090e68:4040/jobs/job?id=30",
              "$$hashKey": "object:5351"
            },
            {
              "jobUrl": "http://277fbf090e68:4040/jobs/job?id=31",
              "$$hashKey": "object:5352"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1618864479752_1241065239",
      "id": "20210323-214005_1459487454",
      "dateCreated": "2021-04-19T20:34:39+0000",
      "status": "FINISHED",
      "$$hashKey": "object:321",
      "dateFinished": "2021-04-19T20:44:17+0000",
      "dateStarted": "2021-04-19T20:44:15+0000"
    },
    {
      "text": "%spark\n// Preferred way if you really just want the top results\n// Note that you do not first need to assign the ordering function to a variable - you could just pass along the Ordering.by expression instead.\nval asc = (Ordering.by[(String, Int), Int](_._2))\nwc.top(10)(asc).foreach(println)",
      "user": "anonymous",
      "dateUpdated": "2021-04-19T20:44:19+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionSupport": true,
          "completionKey": "TAB"
        },
        "editorMode": "ace/mode/scala",
        "colWidth": 12,
        "editorHide": false,
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "(the,25378)\n(I,20629)\n(and,19806)\n(to,16966)\n(of,16718)\n(a,13657)\n(my,11443)\n(in,10519)\n(you,9591)\n(is,8335)\n\u001b[1m\u001b[34masc\u001b[0m: \u001b[1m\u001b[32mscala.math.Ordering[(String, Int)]\u001b[0m = scala.math.Ordering$$anon$7@6eee947a\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://277fbf090e68:4040/jobs/job?id=32",
              "$$hashKey": "object:5392"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1618864479752_1682445909",
      "id": "20210323-214005_1602620819",
      "dateCreated": "2021-04-19T20:34:39+0000",
      "status": "FINISHED",
      "$$hashKey": "object:322",
      "dateFinished": "2021-04-19T20:44:19+0000",
      "dateStarted": "2021-04-19T20:44:19+0000"
    },
    {
      "text": "%spark\n// Alternative formulation\nval desc = (Ordering.by[(String, Int), Int](-_._2))\nwc.takeOrdered(10)(desc).foreach(println)",
      "user": "anonymous",
      "dateUpdated": "2021-04-19T20:44:22+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionSupport": true,
          "completionKey": "TAB"
        },
        "editorMode": "ace/mode/scala",
        "colWidth": 12,
        "editorHide": false,
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "(the,25378)\n(I,20629)\n(and,19806)\n(to,16966)\n(of,16718)\n(a,13657)\n(my,11443)\n(in,10519)\n(you,9591)\n(is,8335)\n\u001b[1m\u001b[34mdesc\u001b[0m: \u001b[1m\u001b[32mscala.math.Ordering[(String, Int)]\u001b[0m = scala.math.Ordering$$anon$7@1a0e68f5\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://277fbf090e68:4040/jobs/job?id=33",
              "$$hashKey": "object:5428"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1618864479752_2016309617",
      "id": "20210323-214005_211949874",
      "dateCreated": "2021-04-19T20:34:39+0000",
      "status": "FINISHED",
      "$$hashKey": "object:323",
      "dateFinished": "2021-04-19T20:44:23+0000",
      "dateStarted": "2021-04-19T20:44:22+0000"
    },
    {
      "text": "%md\nThe next section saves the results of word counting in the filesystem. \n\nWe use a simple shell command to look into the directory that has been created.\n(Alternatively, you can navigate the filesystem after issuing a `docker exec -it HASH bash` command on the machine running the notebook container.)",
      "user": "anonymous",
      "dateUpdated": "2021-04-19T20:34:39+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "colWidth": 12,
        "editorHide": true,
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<p>The next section saves the results of word counting in the filesystem. </p>\n<p>We use a simple shell command to look into the directory that has been created.<br/>(Alternatively, you can navigate the filesystem after issuing a <code>docker exec -it HASH bash</code> command on the machine running the notebook container.)</p>\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1618864479752_958842836",
      "id": "20210323-214005_1647126852",
      "dateCreated": "2021-04-19T20:34:39+0000",
      "status": "READY",
      "$$hashKey": "object:324"
    },
    {
      "text": "%spark\nwords.saveAsTextFile(\"file:///opt/hadoop/wc\")",
      "user": "anonymous",
      "dateUpdated": "2021-04-19T20:44:26+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionSupport": true,
          "completionKey": "TAB"
        },
        "editorMode": "ace/mode/scala",
        "colWidth": 12,
        "editorHide": false,
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://277fbf090e68:4040/jobs/job?id=34",
              "$$hashKey": "object:5448"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1618864479752_1726475335",
      "id": "20210323-214005_1147354925",
      "dateCreated": "2021-04-19T20:34:39+0000",
      "status": "FINISHED",
      "$$hashKey": "object:325",
      "dateFinished": "2021-04-19T20:44:27+0000",
      "dateStarted": "2021-04-19T20:44:26+0000"
    },
    {
      "text": "%sh\nls -al /opt/hadoop/wc",
      "user": "anonymous",
      "dateUpdated": "2021-04-19T20:44:29+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "sh",
          "editOnDblClick": false,
          "completionSupport": false
        },
        "editorMode": "ace/mode/sh",
        "colWidth": 12,
        "editorHide": false,
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "total 8992\ndrwxr-xr-x 2 hadoop hadoop    4096 Apr 19 20:44 .\ndrwx------ 1 hadoop hadoop    4096 Apr 19 20:44 ..\n-rw-r--r-- 1 hadoop hadoop       8 Apr 19 20:44 ._SUCCESS.crc\n-rw-r--r-- 1 hadoop hadoop   35576 Apr 19 20:44 .part-00000.crc\n-rw-r--r-- 1 hadoop hadoop   35668 Apr 19 20:44 .part-00001.crc\n-rw-r--r-- 1 hadoop hadoop       0 Apr 19 20:44 _SUCCESS\n-rw-r--r-- 1 hadoop hadoop 4552655 Apr 19 20:44 part-00000\n-rw-r--r-- 1 hadoop hadoop 4564467 Apr 19 20:44 part-00001\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1618864479752_294026072",
      "id": "20210323-214005_736516263",
      "dateCreated": "2021-04-19T20:34:39+0000",
      "status": "FINISHED",
      "$$hashKey": "object:326",
      "dateFinished": "2021-04-19T20:44:29+0000",
      "dateStarted": "2021-04-19T20:44:29+0000"
    },
    {
      "text": "%md\n_Q: Explain why there are multiple result files._\n\nWe have multiple workers, working on their own partitions, that all write down files when the job is issued, resulting in multiple output files.\n\nInspect the files from the command line in the docker container (using `docker exec`).",
      "user": "anonymous",
      "dateUpdated": "2021-04-19T20:45:23+0000",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionSupport": false
        },
        "editorMode": "ace/mode/markdown",
        "colWidth": 12,
        "editorHide": true,
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<p><em>Q: Explain why there are multiple result files.</em></p>\n<p>We have multiple workers, working on their own partitions, that all write down files when the job is issued, resulting in multiple output files.</p>\n<p>Inspect the files from the command line in the docker container (using <code>docker exec</code>).</p>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1618864479752_1806601288",
      "id": "20210323-214005_923905888",
      "dateCreated": "2021-04-19T20:34:39+0000",
      "status": "FINISHED",
      "$$hashKey": "object:327",
      "dateFinished": "2021-04-19T20:45:23+0000",
      "dateStarted": "2021-04-19T20:45:23+0000"
    },
    {
      "text": "%md\nClean up the directory to save us from the familiar \"assignment 2 headaches\" when rerunning the notebook again later.",
      "user": "anonymous",
      "dateUpdated": "2021-04-19T20:34:39+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "colWidth": 12,
        "editorHide": true,
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<p>Clean up the directory to save us from the familiar &ldquo;assignment 2 headaches&rdquo; when rerunning the notebook again later.</p>\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1618864479752_1505482100",
      "id": "20210323-214005_1014876111",
      "dateCreated": "2021-04-19T20:34:39+0000",
      "status": "READY",
      "$$hashKey": "object:328"
    },
    {
      "text": "%sh\nrm -rf /opt/hadoop/wc",
      "user": "anonymous",
      "dateUpdated": "2021-04-19T20:45:26+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "sh",
          "editOnDblClick": false,
          "completionSupport": false
        },
        "editorMode": "ace/mode/sh",
        "colWidth": 12,
        "editorHide": false,
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1618864479752_396365403",
      "id": "20210323-214005_1631194426",
      "dateCreated": "2021-04-19T20:34:39+0000",
      "status": "FINISHED",
      "$$hashKey": "object:329",
      "dateFinished": "2021-04-19T20:45:26+0000",
      "dateStarted": "2021-04-19T20:45:26+0000"
    },
    {
      "text": "%md\n### How to count?\n\nLet's count words again, and take a careful look to ensure you understand what the program does. If necessary, use a few empty cells to derive intermediate results, instead of running the program all at once. What does `[^a-z]` mean? What about the `$` sign at the end?",
      "user": "anonymous",
      "dateUpdated": "2021-04-19T20:34:39+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "colWidth": 12,
        "editorHide": true,
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h3>How to count?</h3>\n<p>Let&rsquo;s count words again, and take a careful look to ensure you understand what the program does. If necessary, use a few empty cells to derive intermediate results, instead of running the program all at once. What does <code>[^a-z]</code> mean? What about the <code>$</code> sign at the end?</p>\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1618864479753_1328240910",
      "id": "20210323-214005_2021675738",
      "dateCreated": "2021-04-19T20:34:39+0000",
      "status": "READY",
      "$$hashKey": "object:330"
    },
    {
      "text": "%spark\nval words = lines.flatMap(line => line.split(\" \"))\n              .map(w => w.toLowerCase().replaceAll(\"(^[^a-z]+|[^a-z]+$)\", \"\"))\n              .filter(_ != \"\")\n              .map(w => (w,1))\n              .reduceByKey( _ + _ )",
      "user": "anonymous",
      "dateUpdated": "2021-04-19T20:45:30+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionSupport": true,
          "completionKey": "TAB"
        },
        "editorMode": "ace/mode/scala",
        "colWidth": 12,
        "editorHide": false,
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u001b[1m\u001b[34mwords\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.rdd.RDD[(String, Int)]\u001b[0m = ShuffledRDD[74] at reduceByKey at <console>:33\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1618864479753_1729923278",
      "id": "20210323-214005_981530773",
      "dateCreated": "2021-04-19T20:34:39+0000",
      "status": "FINISHED",
      "$$hashKey": "object:331",
      "dateFinished": "2021-04-19T20:45:30+0000",
      "dateStarted": "2021-04-19T20:45:30+0000"
    },
    {
      "text": "%md\nWe count again how many times Macbeth is mentioned by Shakespeare in his complete works.",
      "user": "anonymous",
      "dateUpdated": "2021-04-19T20:34:39+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "colWidth": 12,
        "editorHide": true,
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<p>We count again how many times Macbeth is mentioned by Shakespeare in his complete works.</p>\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1618864479753_671057720",
      "id": "20210323-214005_79812013",
      "dateCreated": "2021-04-19T20:34:39+0000",
      "status": "READY",
      "$$hashKey": "object:332"
    },
    {
      "text": "%spark\nwords.filter(_._1 == \"macbeth\").collect\n  .map({case (w,c) => \"%s occurs %d times\".format(w,c)}).map(println)",
      "user": "anonymous",
      "dateUpdated": "2021-04-19T20:45:33+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionSupport": true,
          "completionKey": "TAB"
        },
        "editorMode": "ace/mode/scala",
        "colWidth": 12,
        "editorHide": false,
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "macbeth occurs 285 times\n\u001b[1m\u001b[34mres77\u001b[0m: \u001b[1m\u001b[32mArray[Unit]\u001b[0m = Array(())\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://277fbf090e68:4040/jobs/job?id=35",
              "$$hashKey": "object:5549"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1618864479753_1649937513",
      "id": "20210323-214005_277295527",
      "dateCreated": "2021-04-19T20:34:39+0000",
      "status": "FINISHED",
      "$$hashKey": "object:333",
      "dateFinished": "2021-04-19T20:45:36+0000",
      "dateStarted": "2021-04-19T20:45:33+0000"
    },
    {
      "text": "%md\nFinal question for today:\n\n_Q: Why are the counts different (and, why are they higher than before, and not lower)?_\nBecause now Macbeth, tis'macbeth, macbeth's  all count for the queried word macbeth.",
      "user": "anonymous",
      "dateUpdated": "2021-04-19T20:47:55+0000",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionSupport": false
        },
        "editorMode": "ace/mode/markdown",
        "colWidth": 12,
        "editorHide": true,
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<p>Final question for today:</p>\n<p><em>Q: Why are the counts different (and, why are they higher than before, and not lower)?</em><br />\nBecause now Macbeth, tis&rsquo;macbeth, macbeth&rsquo;s  all count for the queried word macbeth.</p>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1618864479753_1445896034",
      "id": "20210323-214005_191508140",
      "dateCreated": "2021-04-19T20:34:39+0000",
      "status": "FINISHED",
      "$$hashKey": "object:334",
      "dateFinished": "2021-04-19T20:47:55+0000",
      "dateStarted": "2021-04-19T20:47:55+0000"
    },
    {
      "text": "%md\n",
      "user": "anonymous",
      "dateUpdated": "2021-04-19T20:47:55+0000",
      "progress": 0,
      "config": {
        "colWidth": 12,
        "fontSize": 9,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "editorMode": "ace/mode/markdown"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1618865275271_183232859",
      "id": "paragraph_1618865275271_183232859",
      "dateCreated": "2021-04-19T20:47:55+0000",
      "status": "READY",
      "focus": true,
      "$$hashKey": "object:5566"
    }
  ],
  "name": "A3A",
  "id": "2G3F8WTP2",
  "defaultInterpreterGroup": "spark",
  "version": "0.9.0",
  "noteParams": {},
  "noteForms": {},
  "angularObjects": {},
  "config": {
    "isZeppelinNotebookCronEnable": false,
    "looknfeel": "default",
    "personalizedMode": "false"
  },
  "info": {},
  "path": "/A3A"
}